[
["index.html", "Introduction to Data Analysis with R &amp; Reproducible Data Science Home", " Introduction to Data Analysis with R &amp; Reproducible Data Science Institute for Research in Statistics and its Applications at the University of Minnesota Home With the increasing availability of data with broad applications (and the sheer size of some of these data), it is more important than ever to be able to elucidate trends, decisions, and stories from data. Our team will offer a hands on introduction to Data Science and Statistics using the free and publicly available software R. Assuming no background knowledge of software or Statistics, we will bring you up to speed on some of the most useful, modern, and popular data analysis techniques. This short course is divided into multiple modules. On day one we will explore the basic features of R and the power of R for constructing visualizations, summaries, hypothesis tests, and statistical models from data. The modules on day two will cover a gentle introduction to quantile regression and conclude with an in-depth discussion on best practices for reproducible Data Science research and practice using R Markdown and github. The material herein is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["introduction-to-r-rstudio.html", " 1 Introduction to R &amp; RStudio 1.1 Getting Started 1.2 Basic features 1.3 Getting Help 1.4 Working with data 1.5 Exercises", " 1 Introduction to R &amp; RStudio Author: Alicia Johnson Consider the 4 V’s of Big Data: source: https://commons.wikimedia.org/wiki/File:Defining-big-data1.png This workshop is motivated in part by the increasing need for tools that can be used to elucidate trends, decisions, and stories from data. Whether your data are generated via simulation, collected via a survey, observed in a scientific experiment, scraped from the web, etc, you need software to explore and construct inferences from these data. In this workshop, we’ll use the R statistical software. Why R? it’s free it’s open source it’s flexible / useful for a wide variety of applications it has a huge online community it can be used to create reproducible documents, apps, books, etc. (In fact, this document was constructed within RStudio.) Workshop Outline &amp; Goals Our goal is to provide a quick introduction to the power of and principles behind using R for statistical analysis. You will walk away with a solid foundation upon which you can build in utilizing R for your own research. Day 1: Introduction to R &amp; RStudio Data Visualization Simple Statistics Linear Regression Day 2: Quantile Regression Github in R R Markdown Principles of Reproducible Research reproducible research / outline for workshop 1.1 Getting Started Before the workshop, you were asked to download/update both R &amp; RStudio: Download &amp; install R: https://mirror.las.iastate.edu/CRAN/ Download &amp; install RStudio: https://www.rstudio.com/products/rstudio/download/ Be sure to download the free version. Note that RStudio is an integrated development environment (IDE) for R, combining the power of R with extra automation tools. Once you open RStudio, you’ll see four panes, each serving a different function: 1.2 Basic features Using R as a calculator We can use R as a simple calculator. Try the following: 2 + 3 2 * 3 2^3 (2 + 3)^2 2 + 3^2 Comments Once you start saving your work, it’s helpful to comment your code. To this end, R ignores anything after #. #calculate 3 squared 3^2 Built-in functions R also includes built-in functions to which we supply arguments: function(arguments). sqrt(9) #the sum function calculates the sum of the listed numbers #does the order of arguments matter? sum(2, 3) sum(3, 2) #what does the rep function do? does the order of arguments matter? rep(2, 3) rep(3, 2) #some arguments also have names rep(x=3, times=2) rep(times=2, x=3) #is R case sensitive? eg: can we spell rep as Rep? Rep(2, 3) Assignment We can assign &amp; store R output. #store the result of rep(3, 2) as TwoThrees TwoThrees &lt;- rep(3, 2) #check out the results TwoThrees #do something to the results TwoThrees + 5 #Names cannot include spaces! try this: Two Threes &lt;- rep(3,2) 1.3 Getting Help Curious about what happens if you change R code in some way? Try it! Playing around with a function is the best way to learn about its functionality. Can’t remember the code you used in a past analysis? Search for it under the “History” tab in the upper right hand panel in RStudio. Did you make a mistake and don’t want to retype all of your work? Use the up arrow! You can access &amp; subsequently edit any previous line of code by using the up arrow. Don’t know what a certain function does or how it works? You have a couple of options: Use ? within RStudio to access help files. ?rep Google! There’s a massive RStudio community at http://stackoverflow.com/. If you have a question, somebody’s probably already written about it. 1.4 Working with data The following data were utilized in the fivethirtyeight.com article “The Dollar-And-Cents Case Against Hollywood’s Exclusion of Women” which analyzes movies that do/don’t pass the Bechdel test. A movie passes the test if it meets the following criteria: there are \\(\\ge 2\\) female characters the female characters talk to each other at least 1 time, they talk about something other than a male character Data Structure Tidy data tables have two key features: Each row represents a single observational unit of the sample. Each column represents a variable, ie. an attribute of the cases. The data are not treated like code. There are no extras - no row summaries, column summaries, data entry notes, comments, graphs, etc. All data manipulation should be done within R! All comments about the data collection, variables, etc should be provided in a separate code book. Question: What are the units of observation in the above data? What are the variables? Accessing / Importing Data Data are stored in countless different locations (eg: your computer, Google drive, Wiki, etc) and in countless formats (eg: xls, csv, tables, etc). Luckily for us, the Bechdel data are already stored within R in the fivethirtyeight package. In general, packages developed &amp; shared by R users provide specialized functions and data. If you didn’t install the fivethirtyeight package before the workshop, you’ll need to do so now: install.packages(&quot;fivethirtyeight&quot;, dependencies=TRUE) Once you install the package, you can load the package in your R session and access the bechdel data: library(fivethirtyeight) data(bechdel) You can also access the codebook for these data: ?bechdel Examining Data Structure in R Before we do any data analysis we have to understand the structure of our data. Try the following. #view the data table in a separate panel View(bechdel) #check out the first rows in the console head(bechdel) #obtain the data dimensions: rows x columns dim(bechdel) #get the variable names names(bechdel) Examining Specific Variables #to access a single variable use the $ notation bechdel$budget_2013 bechdel$clean_test #we can determine the levels/categories of category variables levels(factor(bechdel$clean_test)) levels(factor(bechdel$binary)) Subsetting Specific Units of Observation We can obtain a subset of observations that satisfy a criterion defined by a variable within the data set: #subset of movies with a 2013 budget under 1 million dollars Cheap &lt;- subset(bechdel, budget_2013&lt;1000000) dim(Cheap) head(Cheap) #subset of movies that fail the test Failures &lt;- subset(bechdel, binary==&quot;FAIL&quot;) dim(Failures) head(Failures) #subset of movies that EITHER have a budget under 1 million dollars OR fail the test CheapOrFail &lt;- subset(bechdel, budget_2013&lt;1000000 | binary==&quot;FALSE&quot;) dim(CheapOrFail) head(CheapOrFail) #subset of movies that BOTH have a budget under 1 million dollars AND fail the test CheapAndFail &lt;- subset(bechdel, budget_2013&lt;1000000 &amp; binary==&quot;FALSE&quot;) dim(CheapAndFail) head(CheapAndFail) Some useful syntax for subsetting: &lt; (less than), &lt;= (less than or equal to), &gt; (greater than), &gt;= (greater than or equal to), == (equal to) &amp; (and), | (or) 1.5 Exercises Let’s apply the above tools to the US_births_2000_2014 data within the fivethirtyeight package. These data were used in the fivethirtyeight.com article “Some People Are Too Superstitious To Have A Baby On Friday The 13th”. Load the data into your console and examine the codebook. View the data set in a separate panel. Check out the first 6 cases in your console. What are the units of observation (rows)? What are the variables? How much data do we have? What are the names of the variables? Access the day_of_week variable alone. What are the levels/category labels for this variable? Create a subset that contains only the births that occur on Fridays. Store this as OnlyFridays. Find the dimensions of this subset. Create a subset that contains only births in 2014. Store this as Only2014. Find the dimensions of this subset. Solutions: #1 library(fivethirtyeight) data(US_births_2000_2014) #2 View(US_births_2000_2014) #3 head(US_births_2000_2014) #4 #each row = a single day #variables include number of births, day of week, etc on that date #5 dim(US_births_2000_2014) #6 names(US_births_2000_2014) #7 US_births_2000_2014$day_of_week levels(factor(US_births_2000_2014$day_of_week)) #8 OnlyFridays &lt;- subset(US_births_2000_2014, day_of_week==&quot;Fri&quot;) dim(OnlyFridays) #9 Only2014 &lt;- subset(US_births_2000_2014, year==2014) dim(Only2014) "],
["data-visualization.html", " 2 Data Visualization 2.1 ggplot 2.2 Univariate visualizations 2.3 Visualizing Relationships 2.4 Exercises 2.5 Extra", " 2 Data Visualization Author: Alicia Johnson The following data set on the 2016 election is stored as a csv file at https://www.macalester.edu/~ajohns24/data/IMAdata1.csv: This data set combines the county level election results provided by Tony McGovern (shared on github), county level demographic data from the df_county_demographics data set within the choroplethr R package, and historical information about red/blue/purple states. Let’s take a quick look: #use read.csv() to import the csv file election &lt;- read.csv(&quot;https://www.macalester.edu/~ajohns24/data/IMAdata1.csv&quot;) dim(election) #dimensions ## [1] 3143 34 head(election, 2) #first 2 rows ## region total_population percent_white percent_black percent_asian ## 1 1001 54907 76 18 1 ## 2 1003 187114 83 9 1 ## percent_hispanic per_capita_income median_rent median_age fips_code ## 1 2 24571 668 37.5 1001 ## 2 4 26766 693 41.5 1003 ## county total_2008 dem_2008 gop_2008 oth_2008 total_2012 ## 1 Autauga County 23641 6093 17403 145 23909 ## 2 Baldwin County 81413 19386 61271 756 84988 ## dem_2012 gop_2012 oth_2012 total_2016 dem_2016 gop_2016 oth_2016 ## 1 6354 17366 189 24661 5908 18110 643 ## 2 18329 65772 887 94090 18409 72780 2901 ## perdem_2016 perrep_2016 winrep_2016 perdem_2012 perrep_2012 ## 1 0.2396 0.7344 TRUE 0.2658 0.7263 ## 2 0.1957 0.7735 TRUE 0.2157 0.7739 ## winrep_2012 polyname abb StateColor value IncomeBracket ## 1 TRUE alabama AL red TRUE low ## 2 TRUE alabama AL red TRUE high names(election) #variable names ## [1] &quot;region&quot; &quot;total_population&quot; &quot;percent_white&quot; ## [4] &quot;percent_black&quot; &quot;percent_asian&quot; &quot;percent_hispanic&quot; ## [7] &quot;per_capita_income&quot; &quot;median_rent&quot; &quot;median_age&quot; ## [10] &quot;fips_code&quot; &quot;county&quot; &quot;total_2008&quot; ## [13] &quot;dem_2008&quot; &quot;gop_2008&quot; &quot;oth_2008&quot; ## [16] &quot;total_2012&quot; &quot;dem_2012&quot; &quot;gop_2012&quot; ## [19] &quot;oth_2012&quot; &quot;total_2016&quot; &quot;dem_2016&quot; ## [22] &quot;gop_2016&quot; &quot;oth_2016&quot; &quot;perdem_2016&quot; ## [25] &quot;perrep_2016&quot; &quot;winrep_2016&quot; &quot;perdem_2012&quot; ## [28] &quot;perrep_2012&quot; &quot;winrep_2012&quot; &quot;polyname&quot; ## [31] &quot;abb&quot; &quot;StateColor&quot; &quot;value&quot; ## [34] &quot;IncomeBracket&quot; Now that we understand the structure of this data set, we can start to ask some questions: To what degree did Trump support vary from county to county? In what number of counties did Trump win? What’s the relationship between Trump’s 2016 support and Romney’s 2012 support? What’s the relationship between Trump’s support and the “color” of the state in which the county exists? Visualizing the data is the first natural step in answering these questions. Why? Visualizations help us understand what we’re working with: What are the scales of our variables? Are there any outliers, i.e. unusual cases? What are the patterns among our variables? This understanding will inform our next steps: What statistical tool / model is appropriate? Once our analysis is complete, visualizations are a powerful way to communicate our findings and tell a story. 2.1 ggplot We’ll construct visualizations using the ggplot function in RStudio. Though the ggplot learning curve can be steep, its “grammar” is intuitive and generalizable once mastered. The ggplot plotting function is stored in the ggplot2 package: library(ggplot2) The best way to learn about ggplot is to just play around. Don’t worry about memorizing the syntax. Rather, focus on the patterns and potential of their application. There’s a helpful cheat sheet for future reference: GGPLOT CHEAT SHEET 2.2 Univariate visualizations We’ll start with univariate visualizations. Categorical Variables Consider the categorical winrep_2016 variable which indicates whether Trump won the county: levels(factor(election$winrep_2016)) ## [1] &quot;FALSE&quot; &quot;TRUE&quot; A table provides a simple summary of the number of counties that fall into these 2 categories: table(election$winrep_2016) ## ## FALSE TRUE ## 487 2625 A bar chart provides a visualization of this table. Try out the code below that builds up from a simple to a customized bar chart. At each step determine how each piece of code contributes to the plot. #set up a plotting frame ggplot(election, aes(x=winrep_2016)) #add a layer with the bars ggplot(election, aes(x=winrep_2016)) + geom_bar() #add axis labels ggplot(election, aes(x=winrep_2016)) + geom_bar() + labs(x=&quot;Trump win&quot;, y=&quot;Number of counties&quot;) In summary: Quantitative Variables The quantitative perrep_2016 variable summarizes Trump’s percent of the vote in each county. Quantitative variables require different summary tools than categorical variables. We’ll explore 2 methods for graphing quantitative variables: histograms &amp; density plots. Histograms are constructed by (1) dividing up the observed range of the variable into ‘bins’ of equal width; and (2) counting up the number of cases that fall into each bin. Try out the code below. #set up a plotting frame ggplot(election, aes(x=perrep_2016)) #add a histogram layer ggplot(election, aes(x=perrep_2016)) + geom_histogram() #add axis labels ggplot(election, aes(x=perrep_2016)) + geom_histogram() + labs(x=&quot;Trump vote (%)&quot;, y=&quot;Number of counties&quot;) #change the border colors ggplot(election, aes(x=perrep_2016)) + geom_histogram(color=&quot;white&quot;) + labs(x=&quot;Trump vote (%)&quot;, y=&quot;Number of counties&quot;) #change the bin width ggplot(election, aes(x=perrep_2016)) + geom_histogram(color=&quot;white&quot;, binwidth=0.10) + labs(x=&quot;Trump vote (%)&quot;, y=&quot;Number of counties&quot;) In summary: ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Density plots are essentially smooth versions of the histogram. Instead of sorting cases into discrete bins, the “density” of cases is calculated across the entire range of values. The greater the number of cases, the greater the density! The density is then scaled so that the area under the density curve always equals 1 and the area under any fraction of the curve represents the fraction of cases that lie in that range. #set up the plotting frame ggplot(election, aes(x=perrep_2016)) #add a density curve ggplot(election, aes(x=perrep_2016)) + geom_density() #add axis labels ggplot(election, aes(x=perrep_2016)) + geom_density() + labs(x=&quot;Trump vote (%)&quot;) #add a fill color ggplot(election, aes(x=perrep_2016)) + geom_density(fill=&quot;red&quot;) + labs(x=&quot;Trump vote (%)&quot;) In summary: 2.3 Visualizing Relationships Consider the data on just 6 of the counties: Before constructing graphics of the relationships among these variables, we need to understand what features these graphics should have. Without peaking at the exercises, challenge yourself to think about how we might graph the relationships among the following sets of variables: perrep_2016 vs perrep_2012 perrep_2016 vs StateColor perrep_2016 vs perrep_2012 and StateColor (in 1 plot) perrep_2016 vs perrep_2012 and median_rent (in 1 plot) Run through the following exercises which introduce different approaches to visualizing relationships. Scatterplots of 2 quantitative variables Each quantitative variable has an axis. Each case is represented by a dot. #just a graphics frame ggplot(election, aes(y=perrep_2016, x=perrep_2012)) #add a scatterplot layer ggplot(election, aes(y=perrep_2016, x=perrep_2012)) + geom_point() #another predictor ggplot(election, aes(y=perrep_2016, x=median_rent)) + geom_point() In summary: Side-by-side plots of 1 quantitative variable vs 1 categorical variable #density plots by group ggplot(election, aes(x=perrep_2016, fill=StateColor)) + geom_density() #to see better: add transparency ggplot(election, aes(x=perrep_2016, fill=StateColor)) + geom_density(alpha=0.5) #fix the color scale! ggplot(election, aes(x=perrep_2016, fill=StateColor)) + geom_density(alpha=0.5) + scale_fill_manual(values=c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) #to see better: split groups into separate plots ggplot(election, aes(x=perrep_2016, fill=StateColor)) + geom_density(alpha=0.5) + facet_wrap( ~ StateColor) + scale_fill_manual(values=c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) In summary: Scatterplots of 1 quantitative variable vs 1 categorical &amp; 1 quantitative variable If median_rent and StateColor both explain some of the variability in perrep_2016, why not include both in our analysis?! Let’s. #scatterplot: id groups using color ggplot(election, aes(y=perrep_2016, x=median_rent, color=StateColor)) + geom_point(alpha=0.5) #fix the color scale! ggplot(election, aes(y=perrep_2016, x=median_rent, color=StateColor)) + geom_point(alpha=0.5) + scale_color_manual(values=c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) #scatterplot: id groups using shape ggplot(election, aes(y=perrep_2016, x=median_rent, shape=StateColor)) + geom_point(alpha=0.5) + scale_color_manual(values=c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) #scatterplot: split/facet by group ggplot(election, aes(y=perrep_2016, x=median_rent, color=StateColor)) + geom_point(alpha=0.5) + facet_wrap( ~ StateColor) + scale_color_manual(values=c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) In summary: Plots of 3 quantitative variables #scatterplot: represent third variable using color ggplot(election, aes(y=perrep_2016, x=median_rent, color=perrep_2012)) + geom_point(alpha=0.5) #scatterplot: discretize the third variable into 2 groups &amp; represent with color ggplot(election, aes(y=perrep_2016, x=median_rent, color=cut(perrep_2012,2))) + geom_point(alpha=0.5) In summary: 2.4 Exercises Recall the US_births_2000_2014 data in the fivethirtyeight package: library(fivethirtyeight) data(&quot;US_births_2000_2014&quot;) In the previous activity, we investigated the basic features of this data set: dim(US_births_2000_2014) ## [1] 5479 6 head(US_births_2000_2014) ## # A tibble: 6 x 6 ## year month date_of_month date day_of_week births ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;date&gt; &lt;ord&gt; &lt;int&gt; ## 1 2000 1 1 2000-01-01 Sat 9083 ## 2 2000 1 2 2000-01-02 Sun 8006 ## 3 2000 1 3 2000-01-03 Mon 11363 ## 4 2000 1 4 2000-01-04 Tues 13032 ## 5 2000 1 5 2000-01-05 Wed 12558 ## 6 2000 1 6 2000-01-06 Thurs 12466 names(US_births_2000_2014) ## [1] &quot;year&quot; &quot;month&quot; &quot;date_of_month&quot; &quot;date&quot; ## [5] &quot;day_of_week&quot; &quot;births&quot; levels(factor(US_births_2000_2014$day_of_week)) ## [1] &quot;Sun&quot; &quot;Mon&quot; &quot;Tues&quot; &quot;Wed&quot; &quot;Thurs&quot; &quot;Fri&quot; &quot;Sat&quot; Let’s graphically explore these variables and the relationships among them! NOTE: This set of exercises is inspired by the work of Randy Pruim for the MAA statPREP program. First, let’s focus on 2014: Only2014 &lt;- subset(US_births_2000_2014, year==2014) Construct a univariate visualization of births. Describe the variability in births from day to day in 2014. The time of year might explain some of this variability. Construct a plot that illustrates the relationship between births and date in 2014. NOTE: Make sure that births, our variable of interest, is on the y-axis and treat date as quantitative. One goofy thing that stands out are the 2-3 distinct groups of points. Add a layer to this plot that explains the distinction between these groups. There are some exceptions to the rule in exercise 3, ie. some cases that should belong to group 1 but behave like the cases in group 2. Explain why these cases are exceptions - what explains the anomalies / why these are special cases? Next, consider all births from 2000-2014. Construct 1 graphic that illustrates births trends across all of these years. Finally, consider only those births that occur on Fridays: OnlyFridays &lt;- subset(US_births_2000_2014, day_of_week==&quot;Fri&quot;) Define a new variable fri13 that indicates whether the case falls on a Friday in the 13th date of the month: OnlyFridays$fri13 &lt;- (OnlyFridays$date_of_month==13) Construct and comment on a plot of that illustrates the distribution of births among Fridays that fall on &amp; off the 13th. Do you see any evidence of superstition? 2.5 Extra We’ve covered some basic graphics. However, different types of relationships require different visualization strategies. For example, there’s a geographical component to the election data. If you have time, try to construct some maps of the election related variables. To this end, you’ll need to install the choroplethr and choroplethrMaps packages: install.packages(&quot;choroplethr&quot;, dependencies=TRUE) install.packages(&quot;choroplethrMaps&quot;, dependencies=TRUE) library(choroplethr) library(choroplethrMaps) #to make a map of Trump support store `perrep_2016` as value election$value &lt;- election$perrep_2016 county_choropleth(election) #a map of Trump wins election$value &lt;- election$winrep_2016 county_choropleth(election) #a map of state color election$value &lt;- election$StateColor county_choropleth(election) #a map of percent white election$value &lt;- election$percent_white county_choropleth(election) "],
["simple-statistics-in-r.html", " 3 Simple Statistics in R 3.1 Chapter Outline and Goals 3.2 Minnesota Beer Data 3.3 Descriptive Statistics in R 3.4 Student’s t-Test in R 3.5 One-Way ANOVA in R 3.6 Correlation Tests in R 3.7 Exercises", " 3 Simple Statistics in R Author: Nathan Helwig 3.1 Chapter Outline and Goals In this chapter, we will cover how to… Load, explore, and summarize data Calculate descriptive statistics Create reproducible plots and tables Perform one and two sample t-tests Fit one-way analysis of variance models Conduct simple correlation tests R has many helpful functions for simple descriptive and inferential statistics, which make reproducible research easy! 3.2 Minnesota Beer Data 3.2.1 Overview The Minnesota beer data has 44 beers measured on 7 variables: Brewery: Name of the brewery (factor with 8 levels) Beer: Name of the beer (factor with 44 levels) Description: Description of the beer (factor with 37 levels) Style: Style of the beer (factor with 3 levels) ABV: Alcohol by volume (numeric) IBU: International bitterness units (integer) Rating: Beer Advocate rating (integer) Data obtained by NEH from Beer Advocate and the websites of the eight breweries. 3.2.2 Load the Data Use the read.csv function to load the beer data into R beer &lt;- read.csv(&quot;http://users.stat.umn.edu/~helwig/notes/MNbeer.csv&quot;) The dim function returns the number of rows and columns of the data frame dim(beer) ## [1] 44 7 The beer data frame has 44 beers (rows) measured on 7 variables (columns). The names function returns the names of the variables in a data frame names(beer) ## [1] &quot;Brewery&quot; &quot;Beer&quot; &quot;Description&quot; &quot;Style&quot; ## [5] &quot;ABV&quot; &quot;IBU&quot; &quot;Rating&quot; 3.2.3 Look at the Data The head function returns the first six lines of a data frame head(beer) ## Brewery Beer Description Style ABV IBU ## 1 Bauhaus Wonderstuff New Bohemian Pilsner Lager 5.4 48 ## 2 Bauhaus Stargazer German Style Schwarzbier Lager 5.0 28 ## 3 Bauhaus Wagon Party West Cost Style Lager Lager 5.4 55 ## 4 Bauhaus Sky-Five! Midwest Coast IPA IPA 6.7 70 ## 5 Bent Paddle Kanu Session Pale Ale Ale 4.8 48 ## 6 Bent Paddle Venture Pils Pilsner Lager Lager 5.0 38 ## Rating ## 1 88 ## 2 87 ## 3 86 ## 4 86 ## 5 85 ## 6 87 The summary function provides a summary of each variable in a data frame summary(beer) ## Brewery Beer Description ## Indeed :7 14* ESB : 1 India Pale Ale : 5 ## Summit :7 B-Side Pils : 1 English IPA : 2 ## Surly :7 Batch 300 : 1 Pilsner Lager : 2 ## Bent Paddle :5 Bender : 1 Porter : 2 ## Fulton :5 Bent Hop : 1 American Blonde Ale : 1 ## Urban Growler:5 Big Boot Rye IPA: 1 Belgian Style Pale Ale: 1 ## (Other) :8 (Other) :38 (Other) :31 ## Style ABV IBU Rating ## Ale :18 Min. :4.20 Min. :15.0 Min. :79.0 ## IPA :17 1st Qu.:5.20 1st Qu.:33.0 1st Qu.:85.0 ## Lager: 9 Median :5.60 Median :48.5 Median :87.0 ## Mean :5.82 Mean :51.1 Mean :87.2 ## 3rd Qu.:6.50 3rd Qu.:68.2 3rd Qu.:90.0 ## Max. :7.50 Max. :99.0 Max. :98.0 ## 3.2.4 Plot the Data The brewer.pal function in the RColorBrewer package creates ColorBrewer palettes for plotting library(RColorBrewer) MyColors &lt;- brewer.pal(nlevels(beer$Style), &quot;Pastel1&quot;) The boxplot function creates simple boxplots boxplot(ABV ~ Style, data = beer, ylab = &quot;Alcohol By Volume (ABV)&quot;, main = &quot;Alcohol by Style of Beer&quot;, col = MyColors) The plot function creates generic X-Y scatterplots StyleInt &lt;- as.integer(beer$Style) plot(beer$IBU, beer$ABV, xlab = &quot;International Bitterness Units (IBU)&quot;, ylab = &quot;Alcohol By Volume (ABV)&quot;, pch = StyleInt + 14, main = &quot;Bitterness vs Alcohol&quot;, col = MyColors[StyleInt]) legend(&quot;bottomright&quot;, legend = levels(beer$Style), pch = 15:17, col = MyColors, bty = &quot;n&quot;) R has functions for saving plots to various figure formats: bmp function for bitmap graphics jpeg function for JPEG graphics png function for Portable Network Graphics tiff function for Tag Image File Format graphics pdf or dev.copy2pdf functions for Portable Document Format graphics postscript or dev.copy2eps functions for PostScript graphics 3.3 Descriptive Statistics in R 3.3.1 Overview We often need to calculate simple descriptive statistics of variables in a data frame, e.g., to make summary tables. As we have already seen, R is a function based and object oriented programming language. To obtain descriptive statistics, we input an object (e.g., column of a data frame) into the corresponding function. Thankfully, functions in R often have intuitive names—you can typically guess the name of the function you need! 3.3.2 Minimum and Maximum To calculate the minimum or maximum of a variable, we could use the min or max functions min(beer$ABV) ## [1] 4.2 max(beer$ABV) ## [1] 7.5 or the range function to return both the minimum and maximum range(beer$ABV) ## [1] 4.2 7.5 The minimum ABV in the sample is 4.2% and the maximum is 7.5%. To determine which beers have the min/max ABV values, we can use the which.min and which.max functions minmaxID &lt;- c(which.min(beer$ABV), which.max(beer$ABV)) beer[minmaxID,] ## Brewery Beer Description Style ABV IBU Rating ## 12 Indeed Lucy Session Sour Session Sour Ale Ale 4.2 27 86 ## 38 Surly Overrated West Coast IPA IPA 7.5 69 91 3.3.3 Mean, Standard Deviation, and Variance The mean function calculates the sample mean \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\) mean(beer$ABV) ## [1] 5.818 The sd function calculates the sample standard deviation \\(s = \\{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\\}^{1/2}\\) sd(beer$ABV) ## [1] 0.8176 The var function calculates the sample variance \\(s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\\) var(beer$ABV) ## [1] 0.6685 The mean ABV is about 5.82% with a standard deviation of about 0.82% (variance of about 0.67%). 3.3.4 Medians and Quantiles The median function calculates the sample median of a vector median(beer$ABV) ## [1] 5.6 and the quantile function can be used for other quantiles quantile(beer$ABV) ## 0% 25% 50% 75% 100% ## 4.2 5.2 5.6 6.5 7.5 quantile(beer$ABV, probs = seq(0, 1, length=11)) ## 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% ## 4.20 4.86 5.16 5.30 5.42 5.60 6.20 6.40 6.50 6.87 7.50 The median is 5.6% ABV, which implies that half of the beers have at least 5.6% ABV. 3.3.5 Factor Level Information The levels function extracts the levels (i.e., unique values) of a factor variable levels(beer$Style) ## [1] &quot;Ale&quot; &quot;IPA&quot; &quot;Lager&quot; and the nlevels function returns the number of levels of a factor nlevels(beer$Style) ## [1] 3 The 44 beers are classified into one of three Styles: Ale, IPA, or Lager. 3.3.6 Covariances and Correlations The cov function calculates the covariance \\(c = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\\) between two variables cov(beer$ABV, beer$IBU) ## [1] 13.23 or a covariance matrix between the columns of an input data frame cov(beer[, c(&quot;ABV&quot;,&quot;IBU&quot;,&quot;Rating&quot;)]) ## ABV IBU Rating ## ABV 0.6685 13.23 1.511 ## IBU 13.2266 461.18 30.871 ## Rating 1.5106 30.87 13.920 The cor function calculates the correlation \\(r = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\}^{1/2} \\{\\sum_{i=1}^n (y_i - \\bar{y})^2 \\}^{1/2} }\\) between two variables cor(beer$ABV, beer$IBU) ## [1] 0.7533 or a correlation matrix between the columns of an input data frame cor(beer[, c(&quot;ABV&quot;,&quot;IBU&quot;,&quot;Rating&quot;)]) ## ABV IBU Rating ## ABV 1.0000 0.7533 0.4952 ## IBU 0.7533 1.0000 0.3853 ## Rating 0.4952 0.3853 1.0000 3.3.7 Applying Functions to Multiple Variables To apply a function to several columns, we can use the apply function apply(beer[, c(&quot;ABV&quot;,&quot;IBU&quot;,&quot;Rating&quot;)], 2, range) ## ABV IBU Rating ## [1,] 4.2 15 79 ## [2,] 7.5 99 98 apply(beer[, c(&quot;ABV&quot;,&quot;IBU&quot;,&quot;Rating&quot;)], 2, mean) ## ABV IBU Rating ## 5.818 51.068 87.182 3.3.8 Applying Functions at Levels of Factors Use the tapply (ragged apply) function to apply some function to a numeric variable separately at each level of a factor variable. For example, we could apply the range function to the ABV variable separately for each Style of beer tapply(beer$ABV, beer$Style, range) ## $Ale ## [1] 4.2 7.0 ## ## $IPA ## [1] 5.8 7.5 ## ## $Lager ## [1] 4.5 5.4 In the given sample, Ales range from 4.2% to 7% ABV, India Pale Ales range from 5.8% to 7.5% ABV, and Lagers range from 4.5% to 5.4% ABV. 3.3.9 Making a Table Use the cbind (column combine) function in combination with the tapply function to create tables tab1 &lt;- cbind(tapply(beer$Rating, beer$Style, length), tapply(beer$ABV, beer$Style, mean), tapply(beer$ABV, beer$Style, sd), tapply(beer$IBU, beer$Style, mean), tapply(beer$IBU, beer$Style, sd), tapply(beer$Rating, beer$Style, mean), tapply(beer$Rating, beer$Style, sd)) colnames(tab1) &lt;- c(&quot;n&quot;, &quot;ABV.Mean&quot;, &quot;ABV.SD&quot;, &quot;IBU.Mean&quot;, &quot;IBU.SD&quot;, &quot;Rating.Mean&quot;, &quot;Rating.SD&quot;) rtab1 &lt;- round(tab1, 2) rtab1 ## n ABV.Mean ABV.SD IBU.Mean IBU.SD Rating.Mean Rating.SD ## Ale 18 5.49 0.67 39.00 12.28 86.83 3.50 ## IPA 17 6.56 0.46 73.53 10.59 88.18 4.54 ## Lager 9 5.06 0.35 32.78 12.58 86.00 1.87 The write.csv function can be used to save the table write.csv(rtab1, file = &quot;~/Desktop/table1.csv&quot;, row.names = TRUE) After some minor stylistic edits, the table is ready for publication—without having to manually type or copy-paste numbers! The kable function (in the knitr package) includes nicely formatted tables in R Markdown documents library(knitr) kable(rtab1, caption = &quot;Table 1: Sample size (n) and variable means and standard deviations (SD) for each style of beer.&quot;) Table 3.1: Table 1: Sample size (n) and variable means and standard deviations (SD) for each style of beer. n ABV.Mean ABV.SD IBU.Mean IBU.SD Rating.Mean Rating.SD Ale 18 5.49 0.67 39.00 12.28 86.83 3.50 IPA 17 6.56 0.46 73.53 10.59 88.18 4.54 Lager 9 5.06 0.35 32.78 12.58 86.00 1.87 3.4 Student’s t-Test in R 3.4.1 One Sample t-Test Mass produced beers (e.g., Bud Light, Miller Lite, etc.) have 4.2% ABV. Suppose we want to test if Minnesota beers have the same mean ABV as mass produced beers \\[ H_0: \\mu = 4.2 \\quad \\mbox{vs.} \\quad H_1: \\mu \\neq 4.2 \\] where \\(\\mu\\) is the mean ABV, and \\(H_0\\) and \\(H_1\\) denote the null and alternative hypotheses. Assuming that the ABV scores are normally distributed, the t.test function can be used to test the null hypothesis t.test(beer$ABV, mu = 4.2) ## ## One Sample t-test ## ## data: beer$ABV ## t = 13, df = 43, p-value &lt;2e-16 ## alternative hypothesis: true mean is not equal to 4.2 ## 95 percent confidence interval: ## 5.570 6.067 ## sample estimates: ## mean of x ## 5.818 The observed t statistic is \\(t = 13.13\\) with 43 degrees of freedom, resulting in a p-value of essentially zero—we reject \\(H_0\\) using any standard \\(\\alpha\\) level. The sample mean is \\(\\bar{x} = 5.8\\)% ABV and the 95% confidence interval for the \\(\\mu\\) (population mean ABV of Minnesota beers) is 5.6% to 6.1% ABV. If we expect that the Minnesota beers have higher ABV than mass produced beers, i.e., \\[ H_0: \\mu = 4.2 \\quad \\mbox{vs.} \\quad H_1: \\mu &gt; 4.2 \\] we need to adjust the alternative input t.test(beer$ABV, mu = 4.2, alternative = &quot;greater&quot;) ## ## One Sample t-test ## ## data: beer$ABV ## t = 13, df = 43, p-value &lt;2e-16 ## alternative hypothesis: true mean is greater than 4.2 ## 95 percent confidence interval: ## 5.611 Inf ## sample estimates: ## mean of x ## 5.818 The only noteworthy difference is that the confidence interval is now a 95% lower bound for the mean ABV of Minnesota beers, which we expect to be at least 5.6% ABV. Note that changing the alternative also changes the p-value, but for this example we do not notice (because the p-value is so small). 3.4.2 Two Sample t-Test Suppose that we want to test if IPA beers have higher ABV than non-IPAs (Ales and Lagers) \\[ H_0: \\mu_1 = \\mu_2 \\quad \\mbox{vs.} \\quad H_1: \\mu_1 &gt; \\mu_2 \\] where \\(\\mu_1\\) and \\(\\mu_2\\) denote the mean ABV of IPA and non-IPA beers, respectively. To use the t.test function for a two sample t-test, we need to input two vectors beer$IPA &lt;- (beer$Style == &quot;IPA&quot;) t.test(beer$ABV[beer$IPA], beer$ABV[!beer$IPA], alternative = &quot;greater&quot;) ## ## Welch Two Sample t-test ## ## data: beer$ABV[beer$IPA] and beer$ABV[!beer$IPA] ## t = 7.4, df = 41, p-value = 2e-09 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 0.9415 Inf ## sample estimates: ## mean of x mean of y ## 6.565 5.348 The observed t statistic is \\(t = 7.45\\) with 40.53 degrees of freedom, resulting in a p-value of essentially zero—we reject \\(H_0\\) using any standard \\(\\alpha\\) level. The sample mean difference is \\(\\bar{x}_1 - \\bar{x}_2 = 1.22\\)% ABV and the 95% lower-bound confidence interval reveals that we expect IPAs to have at least 0.94% more ABV than non-IPAs. The default uses the Welch version, which does not assume equal variance for the two groups. The var.equal input can be used to change this assumption, which produces the classic two sample t-test t.test(beer$ABV[beer$IPA], beer$ABV[!beer$IPA], alternative = &quot;greater&quot;, var.equal = TRUE) ## ## Two Sample t-test ## ## data: beer$ABV[beer$IPA] and beer$ABV[!beer$IPA] ## t = 7, df = 42, p-value = 8e-09 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 0.9234 Inf ## sample estimates: ## mean of x mean of y ## 6.565 5.348 Note that the observed t-test statistic, p-value, and 95% lower-bound are slightly different, but our conclusion does not change: we expect IPAs to have at least 0.9% more ABV than non-IPAs. 3.5 One-Way ANOVA in R 3.5.1 Omnibus F-Test Extending the previous example, suppose that we want to test if the mean ABV differs for the three Styles of beer \\[ H_0: \\mu_j = \\mu \\mbox{ for all } j \\quad \\mbox{vs.} \\quad H_1: \\mu_j \\neq \\mu \\mbox{ for some } j \\] where \\(\\mu_j\\) denotes the mean ABV of the three Styles of beer: Ales, IPAs, and Lagers. Assuming that the ABV scores a normally distributed, we can use the aov (analysis of variance) function amod &lt;- aov(ABV ~ Style, data = beer) summary(amod) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Style 2 16.6 8.3 28 2.2e-08 *** ## Residuals 41 12.2 0.3 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The observed F statistic is \\(F = 28\\) with 2 numerator and 41 denominator degrees of freedom, resulting in a p-value of essentially zero—we reject \\(H_0\\) using any standard \\(\\alpha\\) level. We conclude that the mean ABV of Minnesota beers depends on the Style of beer, but the results do not directly reveal which Styles significantly differ from one another. 3.5.2 Pairwise Comparisons (Tukey’s HSD) To determine which Styles significantly differ in their mean ABV, we can use Tukey’s Honest Significant Differences (HSD) procedure via the TukeyHSD function TukeyHSD(amod) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = ABV ~ Style, data = beer) ## ## $Style ## diff lwr upr p adj ## IPA-Ale 1.0703 0.6226 1.5180 0.0000 ## Lager-Ale -0.4389 -0.9793 0.1015 0.1313 ## Lager-IPA -1.5092 -2.0548 -0.9635 0.0000 The pairwise comparisons reveal that IPAs have significantly higher mean ABV than Ales. The estimated mean difference is \\(\\hat{\\delta}_1 = \\hat{\\mu}_{2} - \\hat{\\mu}_1 = 1.07\\) with 95% confidence interval \\(\\delta_1 \\in [0.62, 1.52]\\). Lagers and Ales do not significantly differ in mean ABV. The estimated mean difference is \\(\\hat{\\delta}_2 = \\hat{\\mu}_{3} - \\hat{\\mu}_1 = -0.44\\) with 95% confidence interval \\(\\delta_2 \\in [-0.98, 0.10]\\). Lagers have significantly lower mean ABV than IPAs. The estimated mean difference is \\(\\hat{\\delta}_3 = \\hat{\\mu}_{3} - \\hat{\\mu}_2 = -1.51\\) with 95% confidence interval \\(\\delta_3 \\in [-2.05, -0.96]\\). 3.6 Correlation Tests in R Suppose that we want to test if a beer’s ABV and Rating are postively correlated \\[ H_0: \\rho = 0 \\quad \\mbox{vs.} \\quad H_1: \\rho &gt; 0 \\] where \\(\\rho\\) is the population correlation between the ABV and Rating. Assuming that the ABV and Rating variables follow a bivariate normal distribution, we can use the cor.test function cor.test(beer$ABV, beer$Rating, alternative = &quot;greater&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: beer$ABV and beer$Rating ## t = 3.7, df = 42, p-value = 3e-04 ## alternative hypothesis: true correlation is greater than 0 ## 95 percent confidence interval: ## 0.2785 1.0000 ## sample estimates: ## cor ## 0.4952 The estimated sample correlation is \\(r = 0.495\\) and the 95% lower-bound confidence interval reveals that we expect the ABV and Ratings to have a positive correlation of at least \\(\\rho = 0.28\\). 3.7 Exercises Load the Minnesota Beer Data into R. Make a boxplot of the IBUs by Style of beer. Make a scatterplot of the ABV (x-axis) by Rating (y-axis). Calculate some descriptive statistics for the IBU variable. Create a table showing the sample size and variable means and standard deviations for each Brewery. Repeat the t-tests using the IBU variable as the response. Repeat the one-way ANOVA using the IBU variable as the response. Repeat the correlation test using the IBU and Rating variables. "],
["linear-regression-in-r.html", " 4 Linear Regression in R 4.1 Chapter Outline and Goals 4.2 Minnesota Beer Data (Reminder) 4.3 Simple Linear Regression 4.4 Multiple Linear Regression 4.5 Exercises", " 4 Linear Regression in R Author: Nathan Helwig 4.1 Chapter Outline and Goals In this chapter, we will cover how to… Fit simple and multiple linear regression models Test the significance of regression coefficients Plot and interpret the regression results Make predictions from fit regression models R’s lm (linear model) function will be the primary tool used in the chapter. 4.2 Minnesota Beer Data (Reminder) 4.2.1 Overview The Minnesota beer data has 44 beers measured on 7 variables: Brewery: Name of the brewery (factor with 8 levels) Beer: Name of the beer (factor with 44 levels) Description: Description of the beer (factor with 37 levels) Style: Style of the beer (factor with 3 levels) ABV: Alcohol by volume (numeric) IBU: International bitterness units (integer) Rating: Beer Advocate rating (integer) Data obtained by NEH from Beer Advocate and the websites of the eight breweries. 4.2.2 Load and Look at the Data Use the read.csv function to load the beer data into R beer &lt;- read.csv(&quot;http://users.stat.umn.edu/~helwig/notes/MNbeer.csv&quot;) The head function returns the first six lines of a data frame head(beer) ## Brewery Beer Description Style ABV IBU ## 1 Bauhaus Wonderstuff New Bohemian Pilsner Lager 5.4 48 ## 2 Bauhaus Stargazer German Style Schwarzbier Lager 5.0 28 ## 3 Bauhaus Wagon Party West Cost Style Lager Lager 5.4 55 ## 4 Bauhaus Sky-Five! Midwest Coast IPA IPA 6.7 70 ## 5 Bent Paddle Kanu Session Pale Ale Ale 4.8 48 ## 6 Bent Paddle Venture Pils Pilsner Lager Lager 5.0 38 ## Rating ## 1 88 ## 2 87 ## 3 86 ## 4 86 ## 5 85 ## 6 87 4.3 Simple Linear Regression 4.3.1 Fit the Model Consider a simple linear regression model of the form \\[ y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\] where \\(y_i\\) is the Rating of the i-th beer (response), \\(x_i\\) is the ABV of the i-th beer (predictor), \\(\\beta_0\\) is the unknown regression intercept, \\(\\beta_1\\) is the unknown regression slope, and \\(\\epsilon_i \\sim \\mathrm{N}(0, \\sigma^2)\\) is a latent Gaussian error term. To fit the model, we can use the lm function mod &lt;- lm(Rating ~ ABV, data = beer) The first input is the regression formula (Response ~ Predictor), and the second input is the data frame containing the variables in the regression formula. Note that mod is an object of class lm, which is a list containing information about the fit model. class(mod) ## [1] &quot;lm&quot; names(mod) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; For example, the $coefficients element contains the estimated regression coefficients mod$coefficients ## (Intercept) ABV ## 74.03 2.26 which reveal that the expected Rating increases by about 2.26 points for every 1 unit (i.e., 1%) increaese in ABV. 4.3.2 Inference Information To obtain a more detailed summary of the fit model, use the summary function modsum &lt;- summary(mod) names(modsum) ## [1] &quot;call&quot; &quot;terms&quot; &quot;residuals&quot; &quot;coefficients&quot; ## [5] &quot;aliased&quot; &quot;sigma&quot; &quot;df&quot; &quot;r.squared&quot; ## [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot; &quot;cov.unscaled&quot; modsum ## ## Call: ## lm(formula = Rating ~ ABV, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.497 -2.156 0.359 1.667 7.018 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 74.035 3.593 20.60 &lt; 2e-16 *** ## ABV 2.260 0.612 3.69 0.00063 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.28 on 42 degrees of freedom ## Multiple R-squared: 0.245, Adjusted R-squared: 0.227 ## F-statistic: 13.6 on 1 and 42 DF, p-value: 0.000632 Note that summarizing an lm object returns the estimated error standard deviation sigma (\\(\\hat{\\sigma} = 3.28\\)), the coefficient of determination r.squared (\\(R^2 = 0.2452\\)), and a coefficient inference table for testing \\(H_0: \\beta_j = 0\\) versus \\(H_1: \\beta_j \\neq 0\\). The observed t statistic for testing the slope parameter is \\(t = 3.69\\) with 42 degrees of freedom, resulting in a p-value less than 0.001—we reject \\(H_0\\) using any standard \\(\\alpha\\) level. Use the confint function to obtain confidence intervals for regression coefficients confint(mod, &quot;ABV&quot;) ## 2.5 % 97.5 % ## ABV 1.025 3.494 The 95% confidence interval for \\(\\beta_1\\) reveals that we expect the average Rating to increase by 1.03 to 3.49 points for each additional 1% ABV. 4.3.3 Plot the Regression Line The abline function makes it easy to include the least-squares regression line on a scatterplot plot(beer$ABV, beer$Rating, xlab = &quot;Alcohol By Volume&quot;, ylab = &quot;Beer Advocate Rating&quot;, main = &quot;Alcohol by Rating&quot;) abline(mod) 4.3.4 Diagnostic and Influence Plots R makes it really easy to create simple diagnostic and influence plots for a fit regression model: plot(mod) 4.3.5 Predction for New Data We often want to use a fit regression model to create predictions for new data. In R, this involves first creating the data frame of new predictor scores newdata &lt;- data.frame(ABV = seq(4.2, 7.5, by = 0.1)) which we input to the predict function along with the fit model newfit &lt;- predict(mod, newdata) newfit ## 1 2 3 4 5 6 7 8 9 10 11 12 ## 83.53 83.75 83.98 84.20 84.43 84.66 84.88 85.11 85.33 85.56 85.78 86.01 ## 13 14 15 16 17 18 19 20 21 22 23 24 ## 86.24 86.46 86.69 86.91 87.14 87.37 87.59 87.82 88.04 88.27 88.50 88.72 ## 25 26 27 28 29 30 31 32 33 34 ## 88.95 89.17 89.40 89.63 89.85 90.08 90.30 90.53 90.76 90.98 By default, the predict function returns a vector of predictions \\(\\hat{y}_{i(\\mbox{new})} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i(\\mbox{new})}\\). To obtain the corresponding standard errors of the predictions, we can use the se.fit input newfitse &lt;- predict(mod, newdata, se.fit = TRUE) newfitse ## $fit ## 1 2 3 4 5 6 7 8 9 10 11 12 ## 83.53 83.75 83.98 84.20 84.43 84.66 84.88 85.11 85.33 85.56 85.78 86.01 ## 13 14 15 16 17 18 19 20 21 22 23 24 ## 86.24 86.46 86.69 86.91 87.14 87.37 87.59 87.82 88.04 88.27 88.50 88.72 ## 25 26 27 28 29 30 31 32 33 34 ## 88.95 89.17 89.40 89.63 89.85 90.08 90.30 90.53 90.76 90.98 ## ## $se.fit ## 1 2 3 4 5 6 7 8 9 10 ## 1.1065 1.0521 0.9985 0.9459 0.8943 0.8440 0.7952 0.7483 0.7035 0.6614 ## 11 12 13 14 15 16 17 18 19 20 ## 0.6225 0.5873 0.5567 0.5314 0.5121 0.4997 0.4946 0.4970 0.5068 0.5236 ## 21 22 23 24 25 26 27 28 29 30 ## 0.5468 0.5756 0.6092 0.6469 0.6879 0.7317 0.7779 0.8261 0.8758 0.9270 ## 31 32 33 34 ## 0.9793 1.0325 1.0866 1.1414 ## ## $df ## [1] 42 ## ## $residual.scale ## [1] 3.28 The interval input can be used to create confidence and prediction intervals newfitCI &lt;- predict(mod, newdata, interval = &quot;confidence&quot;) newfitPI &lt;- predict(mod, newdata, interval = &quot;prediction&quot;) head(newfitCI) ## fit lwr upr ## 1 83.53 81.29 85.76 ## 2 83.75 81.63 85.87 ## 3 83.98 81.96 85.99 ## 4 84.20 82.29 86.11 ## 5 84.43 82.62 86.23 ## 6 84.66 82.95 86.36 head(newfitPI) ## fit lwr upr ## 1 83.53 76.54 90.51 ## 2 83.75 76.80 90.70 ## 3 83.98 77.06 90.90 ## 4 84.20 77.31 91.09 ## 5 84.43 77.57 91.29 ## 6 84.66 77.82 91.49 The confidence and prediction intervals can be plotted using plot(beer$ABV, beer$Rating, xlab = &quot;Alcohol By Volume&quot;, ylab = &quot;Beer Advocate Rating&quot;, main = &quot;Alcohol by Rating&quot;, ylim = c(75, 100)) lines(newdata$ABV, newfitCI[,1]) lines(newdata$ABV, newfitCI[,2], lty = 2, col = &quot;blue&quot;) lines(newdata$ABV, newfitCI[,3], lty = 2, col = &quot;blue&quot;) lines(newdata$ABV, newfitPI[,2], lty = 3, col = &quot;red&quot;) lines(newdata$ABV, newfitPI[,3], lty = 3, col = &quot;red&quot;) legend(&quot;bottomright&quot;, lty = 1:3, legend = c(&quot;fit&quot;, &quot;95% CI&quot;, &quot;95% PI&quot;), col = c(&quot;black&quot;, &quot;blue&quot;, &quot;red&quot;), bty = &quot;n&quot;) 4.4 Multiple Linear Regression 4.4.1 Overview A multiple linear regression model has the form \\[ y_i = \\beta_0 + \\sum_{j=1}^p \\beta_j x_{ij} + \\epsilon_i \\] where \\(y_i\\) is the response for the \\(i\\)-th observation, \\(x_{ij}\\) is the j-th predictor for the i-th observation, \\(\\beta_0\\) is the unknown regression intercept, \\(\\beta_j\\) is the unknown regression slope for the j-th predictor, and \\(\\epsilon_i \\sim \\mathrm{N}(0, \\sigma^2)\\) is a latent Gaussian error term. Note that \\(\\beta_j\\) gives the expected change in the response variable for a 1-unit change in the j-th predictor variable conditioned on the other predictors, i.e., holding all other predictors constant. 4.4.2 Additive Effects We will start by considering a model predicting the Rating from the additive effects of ABV and Brewery amod &lt;- lm(Rating ~ ABV + Brewery, data = beer) Note that this model allows each Brewery to have a unique regression intercept (Bauhaus is the baseline), but assumes that the slope between ABV and Rating is the same for each Brewery. We can summarize the model using the same approach as before: amodsum &lt;- summary(amod) amodsum ## ## Call: ## lm(formula = Rating ~ ABV + Brewery, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.702 -1.477 0.217 1.207 5.505 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 77.8216 3.2784 23.74 &lt;2e-16 *** ## ABV 1.5873 0.5342 2.97 0.0053 ** ## BreweryBent Paddle 1.0167 1.7600 0.58 0.5672 ## BreweryFulton -2.2786 1.7592 -1.30 0.2037 ## BreweryIndeed 0.8747 1.6455 0.53 0.5984 ## BrewerySteel Toe 0.0596 1.8970 0.03 0.9751 ## BrewerySummit -1.1684 1.6444 -0.71 0.4821 ## BrewerySurly 4.2534 1.6877 2.52 0.0164 * ## BreweryUrban Growler -3.2278 1.7615 -1.83 0.0754 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.62 on 35 degrees of freedom ## Multiple R-squared: 0.598, Adjusted R-squared: 0.506 ## F-statistic: 6.51 on 8 and 35 DF, p-value: 3.5e-05 Compared to the simple linear regression model containing only the ABV predictor, we have noticeably reduced the residual standard deviation estimate sigma (\\(\\hat{\\sigma} = 2.622\\)) and increased the coefficient of (multiple) determination r.squared (\\(R^2 = 0.5979\\)). The anova and Anova functions can be used to test the significance of terms library(car) ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode anova(amod) # Type I (sequential) SS test ## Analysis of Variance Table ## ## Response: Rating ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## ABV 1 147 146.8 21.35 5e-05 *** ## Brewery 7 211 30.2 4.39 0.0014 ** ## Residuals 35 241 6.9 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Anova(amod) # Type II SS test ## Anova Table (Type II tests) ## ## Response: Rating ## Sum Sq Df F value Pr(&gt;F) ## ABV 60.7 1 8.83 0.0053 ** ## Brewery 211.1 7 4.39 0.0014 ** ## Residuals 240.7 35 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that anova tests the effects sequentially (ABV alone, then Brewery given ABV), whereas the Anova function (in the car package) tests the effects conditioned on the other effect (ABV given Brewery, Brewery given ABV). Using the Type II tests from the Anova function, we see that both ABV (\\(F_{1,35} = 8.83, p = 0.005\\)) and Brewery (\\(F_{7,35} = 4.39, p = 0.001\\)) significantly add to the prediction of the beer’s Rating. 4.4.3 Interaction Effects Next we consider a model predicting the Rating from the interaction effects of ABV and Brewery imod &lt;- lm(Rating ~ ABV * Brewery, data = beer) Note that formula notation is shorthand for Rating ~ ABV + Brewery + ABV:Brewery, so this model allows each Brewery to have a unique regression intercept and slope relating ABV and Rating. We can summarize the model using the same approach as before: imodsum &lt;- summary(imod) imodsum ## ## Call: ## lm(formula = Rating ~ ABV * Brewery, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.291 -1.304 -0.047 1.477 4.943 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 90.420 12.410 7.29 6.2e-08 *** ## ABV -0.653 2.192 -0.30 0.77 ## BreweryBent Paddle -17.065 17.824 -0.96 0.35 ## BreweryFulton -18.885 14.606 -1.29 0.21 ## BreweryIndeed -8.170 14.412 -0.57 0.58 ## BrewerySteel Toe -16.653 16.604 -1.00 0.32 ## BrewerySummit -11.251 15.783 -0.71 0.48 ## BrewerySurly -12.048 14.679 -0.82 0.42 ## BreweryUrban Growler -8.655 19.727 -0.44 0.66 ## ABV:BreweryBent Paddle 3.233 3.182 1.02 0.32 ## ABV:BreweryFulton 2.958 2.581 1.15 0.26 ## ABV:BreweryIndeed 1.624 2.527 0.64 0.53 ## ABV:BrewerySteel Toe 2.885 2.784 1.04 0.31 ## ABV:BrewerySummit 1.785 2.807 0.64 0.53 ## ABV:BrewerySurly 2.824 2.511 1.12 0.27 ## ABV:BreweryUrban Growler 1.003 3.428 0.29 0.77 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.81 on 28 degrees of freedom ## Multiple R-squared: 0.63, Adjusted R-squared: 0.431 ## F-statistic: 3.17 on 15 and 28 DF, p-value: 0.00403 Compared to the additive model, we have slightly increased the residual standard deviation estimate sigma (\\(\\hat{\\sigma} = 2.813\\)) and increased the coefficient of (multiple) determination r.squared (\\(R^2 = 0.6297\\)). Use the Anova function to test the signifiance of the effects library(car) Anova(imod) # Type II SS test ## Anova Table (Type II tests) ## ## Response: Rating ## Sum Sq Df F value Pr(&gt;F) ## ABV 60.7 1 7.67 0.0099 ** ## Brewery 211.1 7 3.81 0.0050 ** ## ABV:Brewery 19.0 7 0.34 0.9267 ## Residuals 221.6 28 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The results reveal that the interaction effect is not significant (\\(F_{7,28} = 0.34, p = 0.927\\)), but the main effects of ABV (\\(F_{1,28} = 7.67, p = 0.01\\)) and Brewery (\\(F_{7,28} = 3.81, p = 0.005\\)) are significant at the classic \\(\\alpha = 0.05\\) significance level. 4.4.4 Comparing Fit Models To compare the fit models, we can use the anova function for F-tests anova(mod, amod, imod) ## Analysis of Variance Table ## ## Model 1: Rating ~ ABV ## Model 2: Rating ~ ABV + Brewery ## Model 3: Rating ~ ABV * Brewery ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 42 452 ## 2 35 241 7 211 3.81 0.005 ** ## 3 28 222 7 19 0.34 0.927 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 or the AIC function to extract Akaike’s information criterion AIC(mod, amod, imod) ## df AIC ## mod 3 233.3 ## amod 10 219.6 ## imod 17 230.0 In this case, the F-tests and AIC values suggest that the additive model should be preferred. We conclude that each Brewery has a unique baseline Rating, and increasing the ABV by 1% corresponds to an expected 1.59 point increase in the Rating. 4.5 Exercises Load the Minnesota Beer Data into R. Make a scatterplot of the IBU (x-axis) by Rating (y-axis) Fit a simple linear regression model predicting Rating from IBU. Is there a significant linear relationship between IBU and Rating? Plot the linear relationship, along with 95% confidence and prediction intervals. Fit a multiple linear regression model predicting Rating from the additive effects of IBU and Brewery. Fit a multiple linear regression model predicting Rating from the additive and interaction effects of IBU and Brewery. Considering the models you fit in Ex 3, 7, 8, which do you prefer and why? "]
]
