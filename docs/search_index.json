[
["index.html", "Introduction to Data Analysis with R &amp; Reproducible Data Science Home", " Introduction to Data Analysis with R &amp; Reproducible Data Science Institute for Research in Statistics and its Applications at the University of Minnesota Home With the increasing availability of data with broad applications (and the sheer size of some of these data), it is more important than ever to be able to elucidate trends, decisions, and stories from data. Our team will offer a hands on introduction to Data Science and Statistics using the free and publicly available software R. Assuming no background knowledge of software or Statistics, we will bring you up to speed on some of the most useful, modern, and popular data analysis techniques. This short course is divided into multiple modules. On day one we will explore the basic features of R and the power of R for constructing visualizations, summaries, hypothesis tests, and statistical models from data. The modules on day two will cover a gentle introduction to quantile regression and conclude with an in-depth discussion on best practices for reproducible Data Science research and practice using R Markdown and github. The material herein is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["introduction-to-r-rstudio.html", " 1 Introduction to R &amp; RStudio 1.1 Getting Started 1.2 Basic features 1.3 Getting Help 1.4 Working with data 1.5 Exercises", " 1 Introduction to R &amp; RStudio Author: Alicia Johnson Consider the 4 V’s of Big Data: source: https://commons.wikimedia.org/wiki/File:Defining-big-data1.png This workshop is motivated in part by the increasing need for tools that can be used to elucidate trends, decisions, and stories from data. Whether your data are generated via simulation, collected via a survey, observed in a scientific experiment, scraped from the web, etc, you need software to explore and construct inferences from these data. In this workshop, we’ll use the R statistical software. Why R? it’s free it’s open source it’s flexible / useful for a wide variety of applications it has a huge online community it can be used to create reproducible documents, apps, books, etc. (In fact, this document was constructed within RStudio.) Workshop Outline &amp; Goals Our goal is to provide a quick introduction to the power of and principles behind using R for statistical analysis. You will walk away with a solid foundation upon which you can build in utilizing R for your own research. Day 1: Introduction to R &amp; RStudio Data Visualization Simple Statistics Linear Regression Day 2: Quantile Regression Github in R R Markdown Principles of Reproducible Research reproducible research / outline for workshop 1.1 Getting Started Before the workshop, you were asked to download/update both R &amp; RStudio: Download &amp; install R: https://mirror.las.iastate.edu/CRAN/ Download &amp; install RStudio: https://www.rstudio.com/products/rstudio/download/ Be sure to download the free version. Note that RStudio is an integrated development environment (IDE) for R, combining the power of R with extra automation tools. Once you open RStudio, you’ll see four panes, each serving a different function: 1.2 Basic features Using R as a calculator We can use R as a simple calculator. Try the following: 2 + 3 2 * 3 2^3 (2 + 3)^2 2 + 3^2 Comments Once you start saving your work, it’s helpful to comment your code. To this end, R ignores anything after #. #calculate 3 squared 3^2 Built-in functions R also includes built-in functions to which we supply arguments: function(arguments). sqrt(9) #the sum function calculates the sum of the listed numbers #does the order of arguments matter? sum(2, 3) sum(3, 2) #what does the rep function do? does the order of arguments matter? rep(2, 3) rep(3, 2) #some arguments also have names rep(x=3, times=2) rep(times=2, x=3) #is R case sensitive? eg: can we spell rep as Rep? Rep(2, 3) Assignment We can assign &amp; store R output. #store the result of rep(3, 2) as TwoThrees TwoThrees &lt;- rep(3, 2) #check out the results TwoThrees #do something to the results TwoThrees + 5 #Names cannot include spaces! try this: Two Threes &lt;- rep(3,2) 1.3 Getting Help Curious about what happens if you change R code in some way? Try it! Playing around with a function is the best way to learn about its functionality. Can’t remember the code you used in a past analysis? Search for it under the “History” tab in the upper right hand panel in RStudio. Did you make a mistake and don’t want to retype all of your work? Use the up arrow! You can access &amp; subsequently edit any previous line of code by using the up arrow. Don’t know what a certain function does or how it works? You have a couple of options: Use ? within RStudio to access help files. ?rep Google! There’s a massive RStudio community at http://stackoverflow.com/. If you have a question, somebody’s probably already written about it. 1.4 Working with data The following data were utilized in the fivethirtyeight.com article “The Dollar-And-Cents Case Against Hollywood’s Exclusion of Women” which analyzes movies that do/don’t pass the Bechdel test. A movie passes the test if it meets the following criteria: there are \\(\\ge 2\\) female characters the female characters talk to each other at least 1 time, they talk about something other than a male character Data Structure Tidy data tables have two key features: Each row represents a single observational unit of the sample. Each column represents a variable, ie. an attribute of the cases. The data are not treated like code. There are no extras - no row summaries, column summaries, data entry notes, comments, graphs, etc. All data manipulation should be done within R! All comments about the data collection, variables, etc should be provided in a separate code book. Question: What are the units of observation in the above data? What are the variables? Accessing / Importing Data Data are stored in countless different locations (eg: your computer, Google drive, Wiki, etc) and in countless formats (eg: xls, csv, tables, etc). Luckily for us, the Bechdel data are already stored within R in the fivethirtyeight package. In general, packages developed &amp; shared by R users provide specialized functions and data. If you didn’t install the fivethirtyeight package before the workshop, you’ll need to do so now: install.packages(&quot;fivethirtyeight&quot;, dependencies=TRUE) Once you install the package, you can load the package in your R session and access the bechdel data: library(fivethirtyeight) data(bechdel) You can also access the codebook for these data: ?bechdel Examining Data Structure in R Before we do any data analysis we have to understand the structure of our data. Try the following. #view the data table in a separate panel View(bechdel) #check out the first rows in the console head(bechdel) #obtain the data dimensions: rows x columns dim(bechdel) #get the variable names names(bechdel) Examining Specific Variables #to access a single variable use the $ notation bechdel$budget_2013 bechdel$clean_test #we can determine the levels/categories of category variables levels(factor(bechdel$clean_test)) levels(factor(bechdel$binary)) Subsetting Specific Units of Observation We can obtain a subset of observations that satisfy a criterion defined by a variable within the data set: #subset of movies with a 2013 budget under 1 million dollars Cheap &lt;- subset(bechdel, budget_2013&lt;1000000) dim(Cheap) head(Cheap) #subset of movies that fail the test Failures &lt;- subset(bechdel, binary==&quot;FAIL&quot;) dim(Failures) head(Failures) #subset of movies that EITHER have a budget under 1 million dollars OR fail the test CheapOrFail &lt;- subset(bechdel, budget_2013&lt;1000000 | binary==&quot;FALSE&quot;) dim(CheapOrFail) head(CheapOrFail) #subset of movies that BOTH have a budget under 1 million dollars AND fail the test CheapAndFail &lt;- subset(bechdel, budget_2013&lt;1000000 &amp; binary==&quot;FALSE&quot;) dim(CheapAndFail) head(CheapAndFail) Some useful syntax for subsetting: &lt; (less than), &lt;= (less than or equal to), &gt; (greater than), &gt;= (greater than or equal to), == (equal to) &amp; (and), | (or) 1.5 Exercises Let’s apply the above tools to the US_births_2000_2014 data within the fivethirtyeight package. These data were used in the fivethirtyeight.com article “Some People Are Too Superstitious To Have A Baby On Friday The 13th”. Load the data into your console and examine the codebook. View the data set in a separate panel. Check out the first 6 cases in your console. What are the units of observation (rows)? What are the variables? How much data do we have? What are the names of the variables? Access the day_of_week variable alone. What are the levels/category labels for this variable? Create a subset that contains only the births that occur on Fridays. Store this as OnlyFridays. Find the dimensions of this subset. Create a subset that contains only births in 2014. Store this as Only2014. Find the dimensions of this subset. Solutions: #1 library(fivethirtyeight) data(US_births_2000_2014) #2 View(US_births_2000_2014) #3 head(US_births_2000_2014) #4 #each row = a single day #variables include number of births, day of week, etc on that date #5 dim(US_births_2000_2014) #6 names(US_births_2000_2014) #7 US_births_2000_2014$day_of_week levels(factor(US_births_2000_2014$day_of_week)) #8 OnlyFridays &lt;- subset(US_births_2000_2014, day_of_week==&quot;Fri&quot;) dim(OnlyFridays) #9 Only2014 &lt;- subset(US_births_2000_2014, year==2014) dim(Only2014) "],
["data-visualization.html", " 2 Data Visualization 2.1 ggplot 2.2 Univariate visualizations 2.3 Visualizing Relationships 2.4 Exercises 2.5 Extra", " 2 Data Visualization Author: Alicia Johnson The following data set on the 2016 election is stored as a csv file at https://www.macalester.edu/~ajohns24/data/IMAdata1.csv: This data set combines the county level election results provided by Tony McGovern (shared on github), county level demographic data from the df_county_demographics data set within the choroplethr R package, and historical information about red/blue/purple states. Let’s take a quick look: #use read.csv() to import the csv file election &lt;- read.csv(&quot;https://www.macalester.edu/~ajohns24/data/IMAdata1.csv&quot;) dim(election) #dimensions ## [1] 3143 34 head(election, 2) #first 2 rows ## region total_population percent_white percent_black percent_asian ## 1 1001 54907 76 18 1 ## 2 1003 187114 83 9 1 ## percent_hispanic per_capita_income median_rent median_age fips_code ## 1 2 24571 668 37.5 1001 ## 2 4 26766 693 41.5 1003 ## county total_2008 dem_2008 gop_2008 oth_2008 total_2012 ## 1 Autauga County 23641 6093 17403 145 23909 ## 2 Baldwin County 81413 19386 61271 756 84988 ## dem_2012 gop_2012 oth_2012 total_2016 dem_2016 gop_2016 oth_2016 ## 1 6354 17366 189 24661 5908 18110 643 ## 2 18329 65772 887 94090 18409 72780 2901 ## perdem_2016 perrep_2016 winrep_2016 perdem_2012 perrep_2012 ## 1 0.2396 0.7344 TRUE 0.2658 0.7263 ## 2 0.1957 0.7735 TRUE 0.2157 0.7739 ## winrep_2012 polyname abb StateColor value IncomeBracket ## 1 TRUE alabama AL red TRUE low ## 2 TRUE alabama AL red TRUE high names(election) #variable names ## [1] &quot;region&quot; &quot;total_population&quot; &quot;percent_white&quot; ## [4] &quot;percent_black&quot; &quot;percent_asian&quot; &quot;percent_hispanic&quot; ## [7] &quot;per_capita_income&quot; &quot;median_rent&quot; &quot;median_age&quot; ## [10] &quot;fips_code&quot; &quot;county&quot; &quot;total_2008&quot; ## [13] &quot;dem_2008&quot; &quot;gop_2008&quot; &quot;oth_2008&quot; ## [16] &quot;total_2012&quot; &quot;dem_2012&quot; &quot;gop_2012&quot; ## [19] &quot;oth_2012&quot; &quot;total_2016&quot; &quot;dem_2016&quot; ## [22] &quot;gop_2016&quot; &quot;oth_2016&quot; &quot;perdem_2016&quot; ## [25] &quot;perrep_2016&quot; &quot;winrep_2016&quot; &quot;perdem_2012&quot; ## [28] &quot;perrep_2012&quot; &quot;winrep_2012&quot; &quot;polyname&quot; ## [31] &quot;abb&quot; &quot;StateColor&quot; &quot;value&quot; ## [34] &quot;IncomeBracket&quot; Now that we understand the structure of this data set, we can start to ask some questions: To what degree did Trump support vary from county to county? In what number of counties did Trump win? What’s the relationship between Trump’s 2016 support and Romney’s 2012 support? What’s the relationship between Trump’s support and the “color” of the state in which the county exists? Visualizing the data is the first natural step in answering these questions. Why? Visualizations help us understand what we’re working with: What are the scales of our variables? Are there any outliers, i.e. unusual cases? What are the patterns among our variables? This understanding will inform our next steps: What statistical tool / model is appropriate? Once our analysis is complete, visualizations are a powerful way to communicate our findings and tell a story. 2.1 ggplot We’ll construct visualizations using the ggplot function in RStudio. Though the ggplot learning curve can be steep, its “grammar” is intuitive and generalizable once mastered. The ggplot plotting function is stored in the ggplot2 package: library(ggplot2) The best way to learn about ggplot is to just play around. Don’t worry about memorizing the syntax. Rather, focus on the patterns and potential of their application. There’s a helpful cheat sheet for future reference: GGPLOT CHEAT SHEET 2.2 Univariate visualizations We’ll start with univariate visualizations. Categorical Variables Consider the categorical winrep_2016 variable which indicates whether Trump won the county: levels(factor(election$winrep_2016)) ## [1] &quot;FALSE&quot; &quot;TRUE&quot; A table provides a simple summary of the number of counties that fall into these 2 categories: table(election$winrep_2016) ## ## FALSE TRUE ## 487 2625 A bar chart provides a visualization of this table. Try out the code below that builds up from a simple to a customized bar chart. At each step determine how each piece of code contributes to the plot. #set up a plotting frame ggplot(election, aes(x=winrep_2016)) #add a layer with the bars ggplot(election, aes(x=winrep_2016)) + geom_bar() #add axis labels ggplot(election, aes(x=winrep_2016)) + geom_bar() + labs(x=&quot;Trump win&quot;, y=&quot;Number of counties&quot;) In summary: Quantitative Variables The quantitative perrep_2016 variable summarizes Trump’s percent of the vote in each county. Quantitative variables require different summary tools than categorical variables. We’ll explore 2 methods for graphing quantitative variables: histograms &amp; density plots. Histograms are constructed by (1) dividing up the observed range of the variable into ‘bins’ of equal width; and (2) counting up the number of cases that fall into each bin. Try out the code below. #set up a plotting frame ggplot(election, aes(x=perrep_2016)) #add a histogram layer ggplot(election, aes(x=perrep_2016)) + geom_histogram() #add axis labels ggplot(election, aes(x=perrep_2016)) + geom_histogram() + labs(x=&quot;Trump vote (%)&quot;, y=&quot;Number of counties&quot;) #change the border colors ggplot(election, aes(x=perrep_2016)) + geom_histogram(color=&quot;white&quot;) + labs(x=&quot;Trump vote (%)&quot;, y=&quot;Number of counties&quot;) #change the bin width ggplot(election, aes(x=perrep_2016)) + geom_histogram(color=&quot;white&quot;, binwidth=0.10) + labs(x=&quot;Trump vote (%)&quot;, y=&quot;Number of counties&quot;) In summary: ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Density plots are essentially smooth versions of the histogram. Instead of sorting cases into discrete bins, the “density” of cases is calculated across the entire range of values. The greater the number of cases, the greater the density! The density is then scaled so that the area under the density curve always equals 1 and the area under any fraction of the curve represents the fraction of cases that lie in that range. #set up the plotting frame ggplot(election, aes(x=perrep_2016)) #add a density curve ggplot(election, aes(x=perrep_2016)) + geom_density() #add axis labels ggplot(election, aes(x=perrep_2016)) + geom_density() + labs(x=&quot;Trump vote (%)&quot;) #add a fill color ggplot(election, aes(x=perrep_2016)) + geom_density(fill=&quot;red&quot;) + labs(x=&quot;Trump vote (%)&quot;) In summary: 2.3 Visualizing Relationships Consider the data on just 6 of the counties: Before constructing graphics of the relationships among these variables, we need to understand what features these graphics should have. Without peaking at the exercises, challenge yourself to think about how we might graph the relationships among the following sets of variables: perrep_2016 vs perrep_2012 perrep_2016 vs StateColor perrep_2016 vs perrep_2012 and StateColor (in 1 plot) perrep_2016 vs perrep_2012 and median_rent (in 1 plot) Run through the following exercises which introduce different approaches to visualizing relationships. Scatterplots of 2 quantitative variables Each quantitative variable has an axis. Each case is represented by a dot. #just a graphics frame ggplot(election, aes(y=perrep_2016, x=perrep_2012)) #add a scatterplot layer ggplot(election, aes(y=perrep_2016, x=perrep_2012)) + geom_point() #another predictor ggplot(election, aes(y=perrep_2016, x=median_rent)) + geom_point() In summary: Side-by-side plots of 1 quantitative variable vs 1 categorical variable #density plots by group ggplot(election, aes(x=perrep_2016, fill=StateColor)) + geom_density() #to see better: add transparency ggplot(election, aes(x=perrep_2016, fill=StateColor)) + geom_density(alpha=0.5) #fix the color scale! ggplot(election, aes(x=perrep_2016, fill=StateColor)) + geom_density(alpha=0.5) + scale_fill_manual(values=c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) #to see better: split groups into separate plots ggplot(election, aes(x=perrep_2016, fill=StateColor)) + geom_density(alpha=0.5) + facet_wrap( ~ StateColor) + scale_fill_manual(values=c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) In summary: Scatterplots of 1 quantitative variable vs 1 categorical &amp; 1 quantitative variable If median_rent and StateColor both explain some of the variability in perrep_2016, why not include both in our analysis?! Let’s. #scatterplot: id groups using color ggplot(election, aes(y=perrep_2016, x=median_rent, color=StateColor)) + geom_point(alpha=0.5) #fix the color scale! ggplot(election, aes(y=perrep_2016, x=median_rent, color=StateColor)) + geom_point(alpha=0.5) + scale_color_manual(values=c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) #scatterplot: id groups using shape ggplot(election, aes(y=perrep_2016, x=median_rent, shape=StateColor)) + geom_point(alpha=0.5) + scale_color_manual(values=c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) #scatterplot: split/facet by group ggplot(election, aes(y=perrep_2016, x=median_rent, color=StateColor)) + geom_point(alpha=0.5) + facet_wrap( ~ StateColor) + scale_color_manual(values=c(&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) In summary: Plots of 3 quantitative variables #scatterplot: represent third variable using color ggplot(election, aes(y=perrep_2016, x=median_rent, color=perrep_2012)) + geom_point(alpha=0.5) #scatterplot: discretize the third variable into 2 groups &amp; represent with color ggplot(election, aes(y=perrep_2016, x=median_rent, color=cut(perrep_2012,2))) + geom_point(alpha=0.5) In summary: 2.4 Exercises Recall the US_births_2000_2014 data in the fivethirtyeight package: library(fivethirtyeight) data(&quot;US_births_2000_2014&quot;) In the previous activity, we investigated the basic features of this data set: dim(US_births_2000_2014) ## [1] 5479 6 head(US_births_2000_2014) ## # A tibble: 6 x 6 ## year month date_of_month date day_of_week births ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;date&gt; &lt;ord&gt; &lt;int&gt; ## 1 2000 1 1 2000-01-01 Sat 9083 ## 2 2000 1 2 2000-01-02 Sun 8006 ## 3 2000 1 3 2000-01-03 Mon 11363 ## 4 2000 1 4 2000-01-04 Tues 13032 ## 5 2000 1 5 2000-01-05 Wed 12558 ## 6 2000 1 6 2000-01-06 Thurs 12466 names(US_births_2000_2014) ## [1] &quot;year&quot; &quot;month&quot; &quot;date_of_month&quot; &quot;date&quot; ## [5] &quot;day_of_week&quot; &quot;births&quot; levels(factor(US_births_2000_2014$day_of_week)) ## [1] &quot;Sun&quot; &quot;Mon&quot; &quot;Tues&quot; &quot;Wed&quot; &quot;Thurs&quot; &quot;Fri&quot; &quot;Sat&quot; Let’s graphically explore these variables and the relationships among them! NOTE: This set of exercises is inspired by the work of Randy Pruim for the MAA statPREP program. First, let’s focus on 2014: Only2014 &lt;- subset(US_births_2000_2014, year==2014) Construct a univariate visualization of births. Describe the variability in births from day to day in 2014. The time of year might explain some of this variability. Construct a plot that illustrates the relationship between births and date in 2014. NOTE: Make sure that births, our variable of interest, is on the y-axis and treat date as quantitative. One goofy thing that stands out are the 2-3 distinct groups of points. Add a layer to this plot that explains the distinction between these groups. There are some exceptions to the rule in exercise 3, ie. some cases that should belong to group 1 but behave like the cases in group 2. Explain why these cases are exceptions - what explains the anomalies / why these are special cases? Next, consider all births from 2000-2014. Construct 1 graphic that illustrates births trends across all of these years. Finally, consider only those births that occur on Fridays: OnlyFridays &lt;- subset(US_births_2000_2014, day_of_week==&quot;Fri&quot;) Define a new variable fri13 that indicates whether the case falls on a Friday in the 13th date of the month: OnlyFridays$fri13 &lt;- (OnlyFridays$date_of_month==13) Construct and comment on a plot of that illustrates the distribution of births among Fridays that fall on &amp; off the 13th. Do you see any evidence of superstition? 2.5 Extra We’ve covered some basic graphics. However, different types of relationships require different visualization strategies. For example, there’s a geographical component to the election data. If you have time, try to construct some maps of the election related variables. To this end, you’ll need to install the choroplethr and choroplethrMaps packages: install.packages(&quot;choroplethr&quot;, dependencies=TRUE) install.packages(&quot;choroplethrMaps&quot;, dependencies=TRUE) library(choroplethr) library(choroplethrMaps) #to make a map of Trump support store `perrep_2016` as value election$value &lt;- election$perrep_2016 county_choropleth(election) #a map of Trump wins election$value &lt;- election$winrep_2016 county_choropleth(election) #a map of state color election$value &lt;- election$StateColor county_choropleth(election) #a map of percent white election$value &lt;- election$percent_white county_choropleth(election) "],
["simple-statistics-in-r.html", " 3 Simple Statistics in R 3.1 Chapter Outline and Goals 3.2 Minnesota Beer Data 3.3 Descriptive Statistics in R 3.4 Student’s t-Test in R 3.5 One-Way ANOVA in R 3.6 Correlation Tests in R 3.7 Exercises", " 3 Simple Statistics in R Author: Nathan Helwig 3.1 Chapter Outline and Goals In this chapter, we will cover how to… Load, explore, and summarize data Calculate descriptive statistics Create reproducible plots and tables Perform one and two sample t-tests Fit one-way analysis of variance models Conduct simple correlation tests R has many helpful functions for simple descriptive and inferential statistics, which make reproducible research easy! 3.2 Minnesota Beer Data 3.2.1 Overview The Minnesota beer data has 44 beers measured on 7 variables: Brewery: Name of the brewery (factor with 8 levels) Beer: Name of the beer (factor with 44 levels) Description: Description of the beer (factor with 37 levels) Style: Style of the beer (factor with 3 levels) ABV: Alcohol by volume (numeric) IBU: International bitterness units (integer) Rating: Beer Advocate rating (integer) Data obtained by NEH from Beer Advocate and the websites of the eight breweries. 3.2.2 Load the Data Use the read.csv function to load the beer data into R beer &lt;- read.csv(&quot;http://users.stat.umn.edu/~helwig/notes/MNbeer.csv&quot;) The dim function returns the number of rows and columns of the data frame dim(beer) ## [1] 44 7 The beer data frame has 44 beers (rows) measured on 7 variables (columns). The names function returns the names of the variables in a data frame names(beer) ## [1] &quot;Brewery&quot; &quot;Beer&quot; &quot;Description&quot; &quot;Style&quot; ## [5] &quot;ABV&quot; &quot;IBU&quot; &quot;Rating&quot; 3.2.3 Look at the Data The head function returns the first six lines of a data frame head(beer) ## Brewery Beer Description Style ABV IBU ## 1 Bauhaus Wonderstuff New Bohemian Pilsner Lager 5.4 48 ## 2 Bauhaus Stargazer German Style Schwarzbier Lager 5.0 28 ## 3 Bauhaus Wagon Party West Cost Style Lager Lager 5.4 55 ## 4 Bauhaus Sky-Five! Midwest Coast IPA IPA 6.7 70 ## 5 Bent Paddle Kanu Session Pale Ale Ale 4.8 48 ## 6 Bent Paddle Venture Pils Pilsner Lager Lager 5.0 38 ## Rating ## 1 88 ## 2 87 ## 3 86 ## 4 86 ## 5 85 ## 6 87 The summary function provides a summary of each variable in a data frame summary(beer) ## Brewery Beer Description ## Indeed :7 14* ESB : 1 India Pale Ale : 5 ## Summit :7 B-Side Pils : 1 English IPA : 2 ## Surly :7 Batch 300 : 1 Pilsner Lager : 2 ## Bent Paddle :5 Bender : 1 Porter : 2 ## Fulton :5 Bent Hop : 1 American Blonde Ale : 1 ## Urban Growler:5 Big Boot Rye IPA: 1 Belgian Style Pale Ale: 1 ## (Other) :8 (Other) :38 (Other) :31 ## Style ABV IBU Rating ## Ale :18 Min. :4.20 Min. :15.0 Min. :79.0 ## IPA :17 1st Qu.:5.20 1st Qu.:33.0 1st Qu.:85.0 ## Lager: 9 Median :5.60 Median :48.5 Median :87.0 ## Mean :5.82 Mean :51.1 Mean :87.2 ## 3rd Qu.:6.50 3rd Qu.:68.2 3rd Qu.:90.0 ## Max. :7.50 Max. :99.0 Max. :98.0 ## 3.2.4 Plot the Data The brewer.pal function in the RColorBrewer package creates ColorBrewer palettes for plotting library(RColorBrewer) MyColors &lt;- brewer.pal(nlevels(beer$Style), &quot;Pastel1&quot;) The boxplot function creates simple boxplots boxplot(ABV ~ Style, data = beer, ylab = &quot;Alcohol By Volume (ABV)&quot;, main = &quot;Alcohol by Style of Beer&quot;, col = MyColors) The plot function creates generic X-Y scatterplots StyleInt &lt;- as.integer(beer$Style) plot(beer$IBU, beer$ABV, xlab = &quot;International Bitterness Units (IBU)&quot;, ylab = &quot;Alcohol By Volume (ABV)&quot;, pch = StyleInt + 14, main = &quot;Bitterness vs Alcohol&quot;, col = MyColors[StyleInt]) legend(&quot;bottomright&quot;, legend = levels(beer$Style), pch = 15:17, col = MyColors, bty = &quot;n&quot;) R has functions for saving plots to various figure formats: bmp function for bitmap graphics jpeg function for JPEG graphics png function for Portable Network Graphics tiff function for Tag Image File Format graphics pdf or dev.copy2pdf functions for Portable Document Format graphics postscript or dev.copy2eps functions for PostScript graphics 3.3 Descriptive Statistics in R 3.3.1 Overview We often need to calculate simple descriptive statistics of variables in a data frame, e.g., to make summary tables. As we have already seen, R is a function based and object oriented programming language. To obtain descriptive statistics, we input an object (e.g., column of a data frame) into the corresponding function. Thankfully, functions in R often have intuitive names—you can typically guess the name of the function you need! 3.3.2 Minimum and Maximum To calculate the minimum or maximum of a variable, we could use the min or max functions min(beer$ABV) ## [1] 4.2 max(beer$ABV) ## [1] 7.5 or the range function to return both the minimum and maximum range(beer$ABV) ## [1] 4.2 7.5 The minimum ABV in the sample is 4.2% and the maximum is 7.5%. To determine which beers have the min/max ABV values, we can use the which.min and which.max functions minmaxID &lt;- c(which.min(beer$ABV), which.max(beer$ABV)) beer[minmaxID,] ## Brewery Beer Description Style ABV IBU Rating ## 12 Indeed Lucy Session Sour Session Sour Ale Ale 4.2 27 86 ## 38 Surly Overrated West Coast IPA IPA 7.5 69 91 3.3.3 Mean, Standard Deviation, and Variance The mean function calculates the sample mean \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\) mean(beer$ABV) ## [1] 5.818 The sd function calculates the sample standard deviation \\(s = \\{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\\}^{1/2}\\) sd(beer$ABV) ## [1] 0.8176 The var function calculates the sample variance \\(s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\\) var(beer$ABV) ## [1] 0.6685 The mean ABV is about 5.82% with a standard deviation of about 0.82% (variance of about 0.67%). 3.3.4 Medians and Quantiles The median function calculates the sample median of a vector median(beer$ABV) ## [1] 5.6 and the quantile function can be used for other quantiles quantile(beer$ABV) ## 0% 25% 50% 75% 100% ## 4.2 5.2 5.6 6.5 7.5 quantile(beer$ABV, probs = seq(0, 1, length=11)) ## 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% ## 4.20 4.86 5.16 5.30 5.42 5.60 6.20 6.40 6.50 6.87 7.50 The median is 5.6% ABV, which implies that half of the beers have at least 5.6% ABV. 3.3.5 Factor Level Information The levels function extracts the levels (i.e., unique values) of a factor variable levels(beer$Style) ## [1] &quot;Ale&quot; &quot;IPA&quot; &quot;Lager&quot; and the nlevels function returns the number of levels of a factor nlevels(beer$Style) ## [1] 3 The 44 beers are classified into one of three Styles: Ale, IPA, or Lager. 3.3.6 Covariances and Correlations The cov function calculates the covariance \\(c = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\\) between two variables cov(beer$ABV, beer$IBU) ## [1] 13.23 or a covariance matrix between the columns of an input data frame cov(beer[, c(&quot;ABV&quot;,&quot;IBU&quot;,&quot;Rating&quot;)]) ## ABV IBU Rating ## ABV 0.6685 13.23 1.511 ## IBU 13.2266 461.18 30.871 ## Rating 1.5106 30.87 13.920 The cor function calculates the correlation \\(r = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\}^{1/2} \\{\\sum_{i=1}^n (y_i - \\bar{y})^2 \\}^{1/2} }\\) between two variables cor(beer$ABV, beer$IBU) ## [1] 0.7533 or a correlation matrix between the columns of an input data frame cor(beer[, c(&quot;ABV&quot;,&quot;IBU&quot;,&quot;Rating&quot;)]) ## ABV IBU Rating ## ABV 1.0000 0.7533 0.4952 ## IBU 0.7533 1.0000 0.3853 ## Rating 0.4952 0.3853 1.0000 3.3.7 Applying Functions to Multiple Variables To apply a function to several columns, we can use the apply function apply(beer[, c(&quot;ABV&quot;,&quot;IBU&quot;,&quot;Rating&quot;)], 2, range) ## ABV IBU Rating ## [1,] 4.2 15 79 ## [2,] 7.5 99 98 apply(beer[, c(&quot;ABV&quot;,&quot;IBU&quot;,&quot;Rating&quot;)], 2, mean) ## ABV IBU Rating ## 5.818 51.068 87.182 3.3.8 Applying Functions at Levels of Factors Use the tapply (ragged apply) function to apply some function to a numeric variable separately at each level of a factor variable. For example, we could apply the range function to the ABV variable separately for each Style of beer tapply(beer$ABV, beer$Style, range) ## $Ale ## [1] 4.2 7.0 ## ## $IPA ## [1] 5.8 7.5 ## ## $Lager ## [1] 4.5 5.4 In the given sample, Ales range from 4.2% to 7% ABV, India Pale Ales range from 5.8% to 7.5% ABV, and Lagers range from 4.5% to 5.4% ABV. 3.3.9 Making a Table Use the cbind (column combine) function in combination with the tapply function to create tables tab1 &lt;- cbind(tapply(beer$Rating, beer$Style, length), tapply(beer$ABV, beer$Style, mean), tapply(beer$ABV, beer$Style, sd), tapply(beer$IBU, beer$Style, mean), tapply(beer$IBU, beer$Style, sd), tapply(beer$Rating, beer$Style, mean), tapply(beer$Rating, beer$Style, sd)) colnames(tab1) &lt;- c(&quot;n&quot;, &quot;ABV.Mean&quot;, &quot;ABV.SD&quot;, &quot;IBU.Mean&quot;, &quot;IBU.SD&quot;, &quot;Rating.Mean&quot;, &quot;Rating.SD&quot;) rtab1 &lt;- round(tab1, 2) rtab1 ## n ABV.Mean ABV.SD IBU.Mean IBU.SD Rating.Mean Rating.SD ## Ale 18 5.49 0.67 39.00 12.28 86.83 3.50 ## IPA 17 6.56 0.46 73.53 10.59 88.18 4.54 ## Lager 9 5.06 0.35 32.78 12.58 86.00 1.87 The write.csv function can be used to save the table write.csv(rtab1, file = &quot;~/Desktop/table1.csv&quot;, row.names = TRUE) After some minor stylistic edits, the table is ready for publication—without having to manually type or copy-paste numbers! The kable function (in the knitr package) includes nicely formatted tables in R Markdown documents library(knitr) kable(rtab1, caption = &quot;Table 1: Sample size (n) and variable means and standard deviations (SD) for each style of beer.&quot;) Table 3.1: Table 1: Sample size (n) and variable means and standard deviations (SD) for each style of beer. n ABV.Mean ABV.SD IBU.Mean IBU.SD Rating.Mean Rating.SD Ale 18 5.49 0.67 39.00 12.28 86.83 3.50 IPA 17 6.56 0.46 73.53 10.59 88.18 4.54 Lager 9 5.06 0.35 32.78 12.58 86.00 1.87 3.4 Student’s t-Test in R 3.4.1 One Sample t-Test Mass produced beers (e.g., Bud Light, Miller Lite, etc.) have 4.2% ABV. Suppose we want to test if Minnesota beers have the same mean ABV as mass produced beers \\[ H_0: \\mu = 4.2 \\quad \\mbox{vs.} \\quad H_1: \\mu \\neq 4.2 \\] where \\(\\mu\\) is the mean ABV, and \\(H_0\\) and \\(H_1\\) denote the null and alternative hypotheses. Assuming that the ABV scores are normally distributed, the t.test function can be used to test the null hypothesis t.test(beer$ABV, mu = 4.2) ## ## One Sample t-test ## ## data: beer$ABV ## t = 13, df = 43, p-value &lt;2e-16 ## alternative hypothesis: true mean is not equal to 4.2 ## 95 percent confidence interval: ## 5.570 6.067 ## sample estimates: ## mean of x ## 5.818 The observed t statistic is \\(t = 13.13\\) with 43 degrees of freedom, resulting in a p-value of essentially zero—we reject \\(H_0\\) using any standard \\(\\alpha\\) level. The sample mean is \\(\\bar{x} = 5.8\\)% ABV and the 95% confidence interval for the \\(\\mu\\) (population mean ABV of Minnesota beers) is 5.6% to 6.1% ABV. If we expect that the Minnesota beers have higher ABV than mass produced beers, i.e., \\[ H_0: \\mu = 4.2 \\quad \\mbox{vs.} \\quad H_1: \\mu &gt; 4.2 \\] we need to adjust the alternative input t.test(beer$ABV, mu = 4.2, alternative = &quot;greater&quot;) ## ## One Sample t-test ## ## data: beer$ABV ## t = 13, df = 43, p-value &lt;2e-16 ## alternative hypothesis: true mean is greater than 4.2 ## 95 percent confidence interval: ## 5.611 Inf ## sample estimates: ## mean of x ## 5.818 The only noteworthy difference is that the confidence interval is now a 95% lower bound for the mean ABV of Minnesota beers, which we expect to be at least 5.6% ABV. Note that changing the alternative also changes the p-value, but for this example we do not notice (because the p-value is so small). 3.4.2 Two Sample t-Test Suppose that we want to test if IPA beers have higher ABV than non-IPAs (Ales and Lagers) \\[ H_0: \\mu_1 = \\mu_2 \\quad \\mbox{vs.} \\quad H_1: \\mu_1 &gt; \\mu_2 \\] where \\(\\mu_1\\) and \\(\\mu_2\\) denote the mean ABV of IPA and non-IPA beers, respectively. To use the t.test function for a two sample t-test, we need to input two vectors beer$IPA &lt;- (beer$Style == &quot;IPA&quot;) t.test(beer$ABV[beer$IPA], beer$ABV[!beer$IPA], alternative = &quot;greater&quot;) ## ## Welch Two Sample t-test ## ## data: beer$ABV[beer$IPA] and beer$ABV[!beer$IPA] ## t = 7.4, df = 41, p-value = 2e-09 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 0.9415 Inf ## sample estimates: ## mean of x mean of y ## 6.565 5.348 The observed t statistic is \\(t = 7.45\\) with 40.53 degrees of freedom, resulting in a p-value of essentially zero—we reject \\(H_0\\) using any standard \\(\\alpha\\) level. The sample mean difference is \\(\\bar{x}_1 - \\bar{x}_2 = 1.22\\)% ABV and the 95% lower-bound confidence interval reveals that we expect IPAs to have at least 0.94% more ABV than non-IPAs. The default uses the Welch version, which does not assume equal variance for the two groups. The var.equal input can be used to change this assumption, which produces the classic two sample t-test t.test(beer$ABV[beer$IPA], beer$ABV[!beer$IPA], alternative = &quot;greater&quot;, var.equal = TRUE) ## ## Two Sample t-test ## ## data: beer$ABV[beer$IPA] and beer$ABV[!beer$IPA] ## t = 7, df = 42, p-value = 8e-09 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 0.9234 Inf ## sample estimates: ## mean of x mean of y ## 6.565 5.348 Note that the observed t-test statistic, p-value, and 95% lower-bound are slightly different, but our conclusion does not change: we expect IPAs to have at least 0.9% more ABV than non-IPAs. 3.5 One-Way ANOVA in R 3.5.1 Omnibus F-Test Extending the previous example, suppose that we want to test if the mean ABV differs for the three Styles of beer \\[ H_0: \\mu_j = \\mu \\mbox{ for all } j \\quad \\mbox{vs.} \\quad H_1: \\mu_j \\neq \\mu \\mbox{ for some } j \\] where \\(\\mu_j\\) denotes the mean ABV of the three Styles of beer: Ales, IPAs, and Lagers. Assuming that the ABV scores a normally distributed, we can use the aov (analysis of variance) function amod &lt;- aov(ABV ~ Style, data = beer) summary(amod) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Style 2 16.6 8.3 28 2.2e-08 *** ## Residuals 41 12.2 0.3 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The observed F statistic is \\(F = 28\\) with 2 numerator and 41 denominator degrees of freedom, resulting in a p-value of essentially zero—we reject \\(H_0\\) using any standard \\(\\alpha\\) level. We conclude that the mean ABV of Minnesota beers depends on the Style of beer, but the results do not directly reveal which Styles significantly differ from one another. 3.5.2 Pairwise Comparisons (Tukey’s HSD) To determine which Styles significantly differ in their mean ABV, we can use Tukey’s Honest Significant Differences (HSD) procedure via the TukeyHSD function TukeyHSD(amod) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = ABV ~ Style, data = beer) ## ## $Style ## diff lwr upr p adj ## IPA-Ale 1.0703 0.6226 1.5180 0.0000 ## Lager-Ale -0.4389 -0.9793 0.1015 0.1313 ## Lager-IPA -1.5092 -2.0548 -0.9635 0.0000 The pairwise comparisons reveal that IPAs have significantly higher mean ABV than Ales. The estimated mean difference is \\(\\hat{\\delta}_1 = \\hat{\\mu}_{2} - \\hat{\\mu}_1 = 1.07\\) with 95% confidence interval \\(\\delta_1 \\in [0.62, 1.52]\\). Lagers and Ales do not significantly differ in mean ABV. The estimated mean difference is \\(\\hat{\\delta}_2 = \\hat{\\mu}_{3} - \\hat{\\mu}_1 = -0.44\\) with 95% confidence interval \\(\\delta_2 \\in [-0.98, 0.10]\\). Lagers have significantly lower mean ABV than IPAs. The estimated mean difference is \\(\\hat{\\delta}_3 = \\hat{\\mu}_{3} - \\hat{\\mu}_2 = -1.51\\) with 95% confidence interval \\(\\delta_3 \\in [-2.05, -0.96]\\). 3.6 Correlation Tests in R Suppose that we want to test if a beer’s ABV and Rating are postively correlated \\[ H_0: \\rho = 0 \\quad \\mbox{vs.} \\quad H_1: \\rho &gt; 0 \\] where \\(\\rho\\) is the population correlation between the ABV and Rating. Assuming that the ABV and Rating variables follow a bivariate normal distribution, we can use the cor.test function cor.test(beer$ABV, beer$Rating, alternative = &quot;greater&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: beer$ABV and beer$Rating ## t = 3.7, df = 42, p-value = 3e-04 ## alternative hypothesis: true correlation is greater than 0 ## 95 percent confidence interval: ## 0.2785 1.0000 ## sample estimates: ## cor ## 0.4952 The estimated sample correlation is \\(r = 0.495\\) and the 95% lower-bound confidence interval reveals that we expect the ABV and Ratings to have a positive correlation of at least \\(\\rho = 0.28\\). 3.7 Exercises Load the Minnesota Beer Data into R. Make a boxplot of the IBUs by Style of beer. Make a scatterplot of the ABV (x-axis) by Rating (y-axis). Calculate some descriptive statistics for the IBU variable. Create a table showing the sample size and variable means and standard deviations for each Brewery. Repeat the t-tests using the IBU variable as the response. Repeat the one-way ANOVA using the IBU variable as the response. Repeat the correlation test using the IBU and Rating variables. "],
["linear-regression-in-r.html", " 4 Linear Regression in R 4.1 Chapter Outline and Goals 4.2 Minnesota Beer Data (Reminder) 4.3 Simple Linear Regression 4.4 Multiple Linear Regression 4.5 Exercises", " 4 Linear Regression in R Author: Nathan Helwig 4.1 Chapter Outline and Goals In this chapter, we will cover how to… Fit simple and multiple linear regression models Test the significance of regression coefficients Plot and interpret the regression results Make predictions from fit regression models R’s lm (linear model) function will be the primary tool used in the chapter. 4.2 Minnesota Beer Data (Reminder) 4.2.1 Overview The Minnesota beer data has 44 beers measured on 7 variables: Brewery: Name of the brewery (factor with 8 levels) Beer: Name of the beer (factor with 44 levels) Description: Description of the beer (factor with 37 levels) Style: Style of the beer (factor with 3 levels) ABV: Alcohol by volume (numeric) IBU: International bitterness units (integer) Rating: Beer Advocate rating (integer) Data obtained by NEH from Beer Advocate and the websites of the eight breweries. 4.2.2 Load and Look at the Data Use the read.csv function to load the beer data into R beer &lt;- read.csv(&quot;http://users.stat.umn.edu/~helwig/notes/MNbeer.csv&quot;) The head function returns the first six lines of a data frame head(beer) ## Brewery Beer Description Style ABV IBU ## 1 Bauhaus Wonderstuff New Bohemian Pilsner Lager 5.4 48 ## 2 Bauhaus Stargazer German Style Schwarzbier Lager 5.0 28 ## 3 Bauhaus Wagon Party West Cost Style Lager Lager 5.4 55 ## 4 Bauhaus Sky-Five! Midwest Coast IPA IPA 6.7 70 ## 5 Bent Paddle Kanu Session Pale Ale Ale 4.8 48 ## 6 Bent Paddle Venture Pils Pilsner Lager Lager 5.0 38 ## Rating ## 1 88 ## 2 87 ## 3 86 ## 4 86 ## 5 85 ## 6 87 4.3 Simple Linear Regression 4.3.1 Fit the Model Consider a simple linear regression model of the form \\[ y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\] where \\(y_i\\) is the Rating of the i-th beer (response), \\(x_i\\) is the ABV of the i-th beer (predictor), \\(\\beta_0\\) is the unknown regression intercept, \\(\\beta_1\\) is the unknown regression slope, and \\(\\epsilon_i \\sim \\mathrm{N}(0, \\sigma^2)\\) is a latent Gaussian error term. To fit the model, we can use the lm function mod &lt;- lm(Rating ~ ABV, data = beer) The first input is the regression formula (Response ~ Predictor), and the second input is the data frame containing the variables in the regression formula. Note that mod is an object of class lm, which is a list containing information about the fit model. class(mod) ## [1] &quot;lm&quot; names(mod) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; For example, the $coefficients element contains the estimated regression coefficients mod$coefficients ## (Intercept) ABV ## 74.03 2.26 which reveal that the expected Rating increases by about 2.26 points for every 1 unit (i.e., 1%) increaese in ABV. 4.3.2 Inference Information To obtain a more detailed summary of the fit model, use the summary function modsum &lt;- summary(mod) names(modsum) ## [1] &quot;call&quot; &quot;terms&quot; &quot;residuals&quot; &quot;coefficients&quot; ## [5] &quot;aliased&quot; &quot;sigma&quot; &quot;df&quot; &quot;r.squared&quot; ## [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot; &quot;cov.unscaled&quot; modsum ## ## Call: ## lm(formula = Rating ~ ABV, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.497 -2.156 0.359 1.667 7.018 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 74.035 3.593 20.60 &lt; 2e-16 *** ## ABV 2.260 0.612 3.69 0.00063 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.28 on 42 degrees of freedom ## Multiple R-squared: 0.245, Adjusted R-squared: 0.227 ## F-statistic: 13.6 on 1 and 42 DF, p-value: 0.000632 Note that summarizing an lm object returns the estimated error standard deviation sigma (\\(\\hat{\\sigma} = 3.28\\)), the coefficient of determination r.squared (\\(R^2 = 0.2452\\)), and a coefficient inference table for testing \\(H_0: \\beta_j = 0\\) versus \\(H_1: \\beta_j \\neq 0\\). The observed t statistic for testing the slope parameter is \\(t = 3.69\\) with 42 degrees of freedom, resulting in a p-value less than 0.001—we reject \\(H_0\\) using any standard \\(\\alpha\\) level. Use the confint function to obtain confidence intervals for regression coefficients confint(mod, &quot;ABV&quot;) ## 2.5 % 97.5 % ## ABV 1.025 3.494 The 95% confidence interval for \\(\\beta_1\\) reveals that we expect the average Rating to increase by 1.03 to 3.49 points for each additional 1% ABV. 4.3.3 Plot the Regression Line The abline function makes it easy to include the least-squares regression line on a scatterplot plot(beer$ABV, beer$Rating, xlab = &quot;Alcohol By Volume&quot;, ylab = &quot;Beer Advocate Rating&quot;, main = &quot;Alcohol by Rating&quot;) abline(mod) 4.3.4 Diagnostic and Influence Plots R makes it really easy to create simple diagnostic and influence plots for a fit regression model: plot(mod) 4.3.5 Prediction for New Data We often want to use a fit regression model to create predictions for new data. In R, this involves first creating the data frame of new predictor scores newdata &lt;- data.frame(ABV = seq(4.2, 7.5, by = 0.1)) which we input to the predict function along with the fit model newfit &lt;- predict(mod, newdata) newfit ## 1 2 3 4 5 6 7 8 9 10 11 12 ## 83.53 83.75 83.98 84.20 84.43 84.66 84.88 85.11 85.33 85.56 85.78 86.01 ## 13 14 15 16 17 18 19 20 21 22 23 24 ## 86.24 86.46 86.69 86.91 87.14 87.37 87.59 87.82 88.04 88.27 88.50 88.72 ## 25 26 27 28 29 30 31 32 33 34 ## 88.95 89.17 89.40 89.63 89.85 90.08 90.30 90.53 90.76 90.98 By default, the predict function returns a vector of predictions \\(\\hat{y}_{i(\\mbox{new})} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i(\\mbox{new})}\\). To obtain the corresponding standard errors of the predictions, we can use the se.fit input newfitse &lt;- predict(mod, newdata, se.fit = TRUE) newfitse ## $fit ## 1 2 3 4 5 6 7 8 9 10 11 12 ## 83.53 83.75 83.98 84.20 84.43 84.66 84.88 85.11 85.33 85.56 85.78 86.01 ## 13 14 15 16 17 18 19 20 21 22 23 24 ## 86.24 86.46 86.69 86.91 87.14 87.37 87.59 87.82 88.04 88.27 88.50 88.72 ## 25 26 27 28 29 30 31 32 33 34 ## 88.95 89.17 89.40 89.63 89.85 90.08 90.30 90.53 90.76 90.98 ## ## $se.fit ## 1 2 3 4 5 6 7 8 9 10 ## 1.1065 1.0521 0.9985 0.9459 0.8943 0.8440 0.7952 0.7483 0.7035 0.6614 ## 11 12 13 14 15 16 17 18 19 20 ## 0.6225 0.5873 0.5567 0.5314 0.5121 0.4997 0.4946 0.4970 0.5068 0.5236 ## 21 22 23 24 25 26 27 28 29 30 ## 0.5468 0.5756 0.6092 0.6469 0.6879 0.7317 0.7779 0.8261 0.8758 0.9270 ## 31 32 33 34 ## 0.9793 1.0325 1.0866 1.1414 ## ## $df ## [1] 42 ## ## $residual.scale ## [1] 3.28 The interval input can be used to create confidence and prediction intervals newfitCI &lt;- predict(mod, newdata, interval = &quot;confidence&quot;) newfitPI &lt;- predict(mod, newdata, interval = &quot;prediction&quot;) head(newfitCI) ## fit lwr upr ## 1 83.53 81.29 85.76 ## 2 83.75 81.63 85.87 ## 3 83.98 81.96 85.99 ## 4 84.20 82.29 86.11 ## 5 84.43 82.62 86.23 ## 6 84.66 82.95 86.36 head(newfitPI) ## fit lwr upr ## 1 83.53 76.54 90.51 ## 2 83.75 76.80 90.70 ## 3 83.98 77.06 90.90 ## 4 84.20 77.31 91.09 ## 5 84.43 77.57 91.29 ## 6 84.66 77.82 91.49 The confidence and prediction intervals can be plotted using plot(beer$ABV, beer$Rating, xlab = &quot;Alcohol By Volume&quot;, ylab = &quot;Beer Advocate Rating&quot;, main = &quot;Alcohol by Rating&quot;, ylim = c(75, 100)) lines(newdata$ABV, newfitCI[,1]) lines(newdata$ABV, newfitCI[,2], lty = 2, col = &quot;blue&quot;) lines(newdata$ABV, newfitCI[,3], lty = 2, col = &quot;blue&quot;) lines(newdata$ABV, newfitPI[,2], lty = 3, col = &quot;red&quot;) lines(newdata$ABV, newfitPI[,3], lty = 3, col = &quot;red&quot;) legend(&quot;bottomright&quot;, lty = 1:3, legend = c(&quot;fit&quot;, &quot;95% CI&quot;, &quot;95% PI&quot;), col = c(&quot;black&quot;, &quot;blue&quot;, &quot;red&quot;), bty = &quot;n&quot;) 4.4 Multiple Linear Regression 4.4.1 Overview A multiple linear regression model has the form \\[ y_i = \\beta_0 + \\sum_{j=1}^p \\beta_j x_{ij} + \\epsilon_i \\] where \\(y_i\\) is the response for the \\(i\\)-th observation, \\(x_{ij}\\) is the j-th predictor for the i-th observation, \\(\\beta_0\\) is the unknown regression intercept, \\(\\beta_j\\) is the unknown regression slope for the j-th predictor, and \\(\\epsilon_i \\sim \\mathrm{N}(0, \\sigma^2)\\) is a latent Gaussian error term. Note that \\(\\beta_j\\) gives the expected change in the response variable for a 1-unit change in the j-th predictor variable conditioned on the other predictors, i.e., holding all other predictors constant. 4.4.2 Additive Effects We will start by considering a model predicting the Rating from the additive effects of ABV and Brewery amod &lt;- lm(Rating ~ ABV + Brewery, data = beer) Note that this model allows each Brewery to have a unique regression intercept (Bauhaus is the baseline), but assumes that the slope between ABV and Rating is the same for each Brewery. We can summarize the model using the same approach as before: amodsum &lt;- summary(amod) amodsum ## ## Call: ## lm(formula = Rating ~ ABV + Brewery, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.702 -1.477 0.217 1.207 5.505 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 77.8216 3.2784 23.74 &lt;2e-16 *** ## ABV 1.5873 0.5342 2.97 0.0053 ** ## BreweryBent Paddle 1.0167 1.7600 0.58 0.5672 ## BreweryFulton -2.2786 1.7592 -1.30 0.2037 ## BreweryIndeed 0.8747 1.6455 0.53 0.5984 ## BrewerySteel Toe 0.0596 1.8970 0.03 0.9751 ## BrewerySummit -1.1684 1.6444 -0.71 0.4821 ## BrewerySurly 4.2534 1.6877 2.52 0.0164 * ## BreweryUrban Growler -3.2278 1.7615 -1.83 0.0754 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.62 on 35 degrees of freedom ## Multiple R-squared: 0.598, Adjusted R-squared: 0.506 ## F-statistic: 6.51 on 8 and 35 DF, p-value: 3.5e-05 Compared to the simple linear regression model containing only the ABV predictor, we have noticeably reduced the residual standard deviation estimate sigma (\\(\\hat{\\sigma} = 2.622\\)) and increased the coefficient of (multiple) determination r.squared (\\(R^2 = 0.5979\\)). The anova and Anova functions can be used to test the significance of terms library(car) ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode anova(amod) # Type I (sequential) SS test ## Analysis of Variance Table ## ## Response: Rating ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## ABV 1 147 146.8 21.35 5e-05 *** ## Brewery 7 211 30.2 4.39 0.0014 ** ## Residuals 35 241 6.9 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Anova(amod) # Type II SS test ## Anova Table (Type II tests) ## ## Response: Rating ## Sum Sq Df F value Pr(&gt;F) ## ABV 60.7 1 8.83 0.0053 ** ## Brewery 211.1 7 4.39 0.0014 ** ## Residuals 240.7 35 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that anova tests the effects sequentially (ABV alone, then Brewery given ABV), whereas the Anova function (in the car package) tests the effects conditioned on the other effect (ABV given Brewery, Brewery given ABV). Using the Type II tests from the Anova function, we see that both ABV (\\(F_{1,35} = 8.83, p = 0.005\\)) and Brewery (\\(F_{7,35} = 4.39, p = 0.001\\)) significantly add to the prediction of the beer’s Rating. 4.4.3 Interaction Effects Next we consider a model predicting the Rating from the interaction effects of ABV and Brewery imod &lt;- lm(Rating ~ ABV * Brewery, data = beer) Note that formula notation is shorthand for Rating ~ ABV + Brewery + ABV:Brewery, so this model allows each Brewery to have a unique regression intercept and slope relating ABV and Rating. We can summarize the model using the same approach as before: imodsum &lt;- summary(imod) imodsum ## ## Call: ## lm(formula = Rating ~ ABV * Brewery, data = beer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.291 -1.304 -0.047 1.477 4.943 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 90.420 12.410 7.29 6.2e-08 *** ## ABV -0.653 2.192 -0.30 0.77 ## BreweryBent Paddle -17.065 17.824 -0.96 0.35 ## BreweryFulton -18.885 14.606 -1.29 0.21 ## BreweryIndeed -8.170 14.412 -0.57 0.58 ## BrewerySteel Toe -16.653 16.604 -1.00 0.32 ## BrewerySummit -11.251 15.783 -0.71 0.48 ## BrewerySurly -12.048 14.679 -0.82 0.42 ## BreweryUrban Growler -8.655 19.727 -0.44 0.66 ## ABV:BreweryBent Paddle 3.233 3.182 1.02 0.32 ## ABV:BreweryFulton 2.958 2.581 1.15 0.26 ## ABV:BreweryIndeed 1.624 2.527 0.64 0.53 ## ABV:BrewerySteel Toe 2.885 2.784 1.04 0.31 ## ABV:BrewerySummit 1.785 2.807 0.64 0.53 ## ABV:BrewerySurly 2.824 2.511 1.12 0.27 ## ABV:BreweryUrban Growler 1.003 3.428 0.29 0.77 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.81 on 28 degrees of freedom ## Multiple R-squared: 0.63, Adjusted R-squared: 0.431 ## F-statistic: 3.17 on 15 and 28 DF, p-value: 0.00403 Compared to the additive model, we have slightly increased the residual standard deviation estimate sigma (\\(\\hat{\\sigma} = 2.813\\)) and increased the coefficient of (multiple) determination r.squared (\\(R^2 = 0.6297\\)). Use the Anova function to test the signifiance of the effects library(car) Anova(imod) # Type II SS test ## Anova Table (Type II tests) ## ## Response: Rating ## Sum Sq Df F value Pr(&gt;F) ## ABV 60.7 1 7.67 0.0099 ** ## Brewery 211.1 7 3.81 0.0050 ** ## ABV:Brewery 19.0 7 0.34 0.9267 ## Residuals 221.6 28 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The results reveal that the interaction effect is not significant (\\(F_{7,28} = 0.34, p = 0.927\\)), but the main effects of ABV (\\(F_{1,28} = 7.67, p = 0.01\\)) and Brewery (\\(F_{7,28} = 3.81, p = 0.005\\)) are significant at the classic \\(\\alpha = 0.05\\) significance level. 4.4.4 Comparing Fit Models To compare the fit models, we can use the anova function for F-tests anova(mod, amod, imod) ## Analysis of Variance Table ## ## Model 1: Rating ~ ABV ## Model 2: Rating ~ ABV + Brewery ## Model 3: Rating ~ ABV * Brewery ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 42 452 ## 2 35 241 7 211 3.81 0.005 ** ## 3 28 222 7 19 0.34 0.927 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 or the AIC function to extract Akaike’s information criterion AIC(mod, amod, imod) ## df AIC ## mod 3 233.3 ## amod 10 219.6 ## imod 17 230.0 In this case, the F-tests and AIC values suggest that the additive model should be preferred. We conclude that each Brewery has a unique baseline Rating, and increasing the ABV by 1% corresponds to an expected 1.59 point increase in the Rating. 4.5 Exercises Load the Minnesota Beer Data into R. Make a scatterplot of the IBU (x-axis) by Rating (y-axis) Fit a simple linear regression model predicting Rating from IBU. Is there a significant linear relationship between IBU and Rating? Plot the linear relationship, along with 95% confidence and prediction intervals. Fit a multiple linear regression model predicting Rating from the additive effects of IBU and Brewery. Fit a multiple linear regression model predicting Rating from the additive and interaction effects of IBU and Brewery. Considering the models you fit in Ex 3, 7, 8, which do you prefer and why? "],
["git-version-control.html", " 5 Git (Version Control) 5.1 License 5.2 R 5.3 Version Control 5.4 Git", " 5 Git (Version Control) Author: Charles J. Geyer 5.1 License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License (http://creativecommons.org/licenses/by-sa/4.0/). 5.2 R The version of R used to make this document is 3.3.2. The version of the rmarkdown package used to make this document is 1.5. The version of the knitr package used to make this document is 1.16. 5.3 Version Control 5.3.1 What is It? revision control version control version control system (VCS) source code control source code management (SCM) content tracking all mean the same thing. It isn’t just for source code. It’s for any “content.” I use it for classes (slides, handouts, homework assignments, tests, solutions), papers (versions of the paper, tech reports, data, R scripts for data analysis), R packages (the traditional source code control, although there is a lot besides source code in an R package), notes (long before they turn into papers, I put them under version control). 5.3.2 Very Old Fashioned Version Control asterLA.7-3-09.tex asterLA.10-3.tex asterLA.7-16.tex asterLA.10-3c.tex asterLA.7-16c.tex asterLA10-4.tex asterLA.7-19.tex asterLA10-4c.tex asterLA.7-19c.tex asterLA10-4d.tex asterLA.7-19z.tex asterLA01-12.tex asterLA8-19.tex asterLA01-12c.tex asterLA8-19c.tex asterLA01-20.tex asterLA.9-1.tex asterLA01-20c.tex asterLA.9-1c.tex asterLA02-22.tex asterLA.9-11.tex A real example, file names of versions of a paper I wrote with a co-author without version control. This works but is highly sub-optimal. Only one author can work at a time because there is no good way to merge simultaneous changes. Nor is there any obvious ownership of the versions. It is hard to tell who did what when. 5.3.3 A Plethora of Version Control Systems Ripped off from Wikipedia (under “Version control”). Local Only Client-Server Distributed SCCS (1972) CVS (1986) BitKeeper (1998) RCS (1982) ClearCase (1992) GNU arch (2001) Perforce (1995) Darcs (2002) Subversion (2000) Monotone (2003) Bazaar (2005) Git (2005) Mercurial (2005) There were more, but I’ve only kept the ones I’d heard of. Don’t worry. We’re only going to talk about one (git). 5.3.4 Mindshare Starting from nowhere in 2005, git has gotten dominant mindshare, at least among free and open source systems. In Google Trends, the only searches that are trending up are for git and github. Searches for competing version control systems are trending down. Many well known open-source projects are on git. The Linux kernel (of course since Linus Torvalds wrote git to be the VCS for the kernel), android (as in phones), Ruby on Rails, Gnome, Qt, KDE, X, Perl, and GNU Emacs. Some aren’t. R is still on Subversion. Firefox, Go, Python, Vim are on Mercurial. 5.3.5 Why? Section 28.1.1.1 of the GNU Emacs manual. Version control systems provide you with three important capabilities: Reversibility: the ability to back up to a previous state if you discover that some modification you did was a mistake or a bad idea. Concurrency: the ability to have many people modifying the same collection of files knowing that conflicting modifications can be detected and resolved. History: the ability to attach historical data to your data, such as explanatory comments about the intention behind each change to it. Even for a programmer working solo, change histories are an important aid to memory; for a multi-person project, they are a vitally important form of communication among developers. 5.4 Git 5.4.1 Getting Git Type ``git’’ into Google and follow the first link it gives you http://git-scm.com/. Under “downloads” it tells you how to get git for Windows and for Mac OS X. If you have Linux, it just comes with (use the installer for your distribution). E. g., on Ubuntu sudo apt-get update sudo apt-get install git 5.4.2 What? From Wikipedia under Git: Torvalds has quipped about the name git, which is British English slang meaning “unpleasant person.” Torvalds said: “I’m an egotistical bastard, and I name all my projects after myself. First ‘Linux’, now ‘git’.” The man page describes Git as “the stupid content tracker”. From memory: Linus has also said that it was a short name that hadn’t already been used for a UNIX command. 5.4.3 Using Git 5.4.3.1 From the Command Line 5.4.3.2 RStudio "],
["an-r-markdown-demo.html", " 6 An R Markdown Demo 6.1 License 6.2 R 6.3 Rendering 6.4 Markdown 6.5 R Markdown 6.6 Examples Not Involving R 6.7 Examples Involving R 6.8 LaTeX Math 6.9 Caching Computation 6.10 Summary", " 6 An R Markdown Demo Author: Charles J. Geyer 6.1 License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License (http://creativecommons.org/licenses/by-sa/4.0/). 6.2 R The version of R used to make this document is 3.3.2. The version of the rmarkdown package used to make this document is 1.5. The version of the knitr package used to make this document is 1.16. 6.3 Rendering This is a demo for using the R package rmarkdown. To turn this file into HTML, use the R commands library(&quot;rmarkdown&quot;) render(&quot;rmark.Rmd&quot;) The same rendering can be accomplished in RStudio by loading the document into Rstudio and clicking the “Knit” button (this also does not seem to need the rmarkdown package). If instead you wish to make a PDF document or some other output format (many are possible), use the optional argument output_format to the render function. library(&quot;rmarkdown&quot;) render(&quot;rmark.Rmd&quot;, output_format=&quot;pdf_document&quot;) Many other output formats are explained in the Rmarkdown documentation. 6.4 Markdown Markdown (Wikipedia page) is “markup language” like HTML and LaTeX but much simpler. Variants of the original markdown language are used by GitHub (https://github.com) for formatting README files and other documentation, by reddit (https://www.reddit.com/) for formatting posts and comments, and by R Markdown (http://rmarkdown.rstudio.com/) for making documents that contain R computations and graphics. Markdown is fast becoming an internet standard for “easy” markup. xkcd:927 Standards Markdown is way too simple to replace HTML or LaTeX, but it gets the job done, even if the result isn’t as pretty as one might like. 6.5 R Markdown 6.5.1 Look If you really need your documents to look exactly the way you want them to look, then you will have to learn LaTeX and knitr. But if you are willing to accept the look that R Markdown gives you, then it is satisfactory for most purposes. 6.5.2 What Does It Do? R Markdown allows you to include R computations and graphics in documents reproducibly. That means that anyone anywhere who gets your R markdown file can satisfy themselves that R did you you say it did. This is very different from the old fashioned way of putting results in documents, where one just cut-and-pastes (snarf-and-barfs) the results into the document. Then there is zero evidence that these numbers or figures were produced the way you claim. For concreteness, here is a simple example. 2 * pi ## [1] 6.283 The result here was not cut-and-pasted into the document. Instead the R expression 2 * pi was executed in R and the result was put in the document by R Markdown, automatically. This happens every time the document is generated so the result always matches the R code that generated it. There is no way they can fail to match (which can happen and often does happen with snarf-and-barf). Here is another concrete example. # set.seed(42) # uncomment to always get the same plot hist(rnorm(1000), probability = TRUE) curve(dnorm, add = TRUE) Every time the document is generated, this figure is different because the random sample produced by rnorm(1000) is different. (If I wanted it to be the same, I could uncomment the set.seed(42) statement.) Again, there is no way the figure can fail to match the R code that generates it. The source code for the document (the Rmd file) is proof that everything is as the document claims. Anyone can verify the results by generating the document again. 6.5.3 Why Do We Want It? 6.5.3.1 The “Replication Crisis” Many are now saying there is a “replication crisis” in science (Wikipedia page). There are many issues affecting this “crisis.” There are scientific issues, such has what experiments are done and how they are interpreted. There are statistical issues, such as too small sample sizes and publication bias. And there are computational issues: what data analysis was done and was it correct? R Markdown only directly helps with the computational issues. It also helps to document your statistical analyses (Duh! Do you think generating a document that includes a statistical analysis may help to document it?) Indirectly, R Markdown also helps with the other issues. Having the whole analysis from raw data to scientific findings publicly available — as many scientific journals now require — tends to make you a lot more careful in doing the analysis. 6.5.3.2 Business and Consulting Outside of science there is no buzz about “replication crisis,” at least not yet. The hype about “big data” is so strong that hardly **anyone is questioning whether results are correct or actually support conclusions people draw from them. But even if the results are never made public, R Markdown can still help a lot. Imagine you have spent weeks doing a very complicated analysis. The day before it is finished, a co-worker tells you there is a mistake in the data that has to be fixed. If you are generating your report the old-fashioned way, it will take almost as much time to redo the report as to do it in the first place. If you have done the report with R Markdown, correct the data, rerun R Markdown, and you’re done. R Markdown takes some time to learn. And it always takes more time to do a data analysis while simultaneously documenting it than to do it while skipping the documentation. But it takes a lot less time than doing the analysis without documentation and adding documentation later. Also R Markdown documentation is far more likely to be correct, and the analysis itself is far more likely to be correct. 6.5.3.3 Newbie Data Analysis The way most newbies use R or any other statistical package is to dive right in typing commands into R, typing commands into a file and cut-and-pasting them into R, or using RStudio. None of these actually document what was done because commands get edited a lot. If you are in the habit of saving your workspace when you leave R or RStudio, can you explain exactly how every R object in there was created? Starting from raw data? Probably not. 6.5.3.4 Expert Data Analysis The way experts use R is type commands into a file, say foo.R. Use R CMD BATCH --vanilla foo.R to run R to do the analysis. type commands with explanations into an R Markdown file, and render it in a clean R environment (empty global environment). Either start R with a clean global environment (with R --vanilla) and do library(&quot;rmarkdown&quot;) render(&quot;foo.Rmd&quot;) or start RStudio with a clean global environment (on the “Tools” menu, select “Global Options” and uncheck “Restore .RData into workspace at startup”, then close and restart) load the R Markdown file and click “Knit”. The important thing is using a clean R environment so all calculations are fully reproducible. Same results every time the analysis is rerun by you or by anybody, anywhere, on any computer that has R. 6.6 Examples Not Involving R One of the purposes of this document is to serve as an example of how to use R Markdown. We have already exemplified a lot, but there’s more. 6.6.1 Title, Author, Date This document shows how to put in a title, author, and date using the YAML comment block (set off by lines of hyphens at the top of the document). There does not seem to be any reference where all of the stuff that can be put in a YAML comment is documented. What can be done with YAML comments is scattered in many places in the R Markdown documentation. 6.6.2 Headers Headers are indicated by hash marks (one for each level), as with all the headers in this document. There is also an alternative format that you can use if you want. 6.6.3 Paragraphs Paragraph breaks are indicated by blank lines. 6.6.4 Fonts Font changes are indicated by stars for italic, double stars for boldface, and backticks for typewriter font (for code) as here. 6.6.5 Lists Bulleted lists examples are shown above. Here is a numbered list item one, item two, and item three. Note that one does not have to do the numbers oneself. R Markdown figures them out (so when you insert a new item in the list it gets the numbers right without you doing anything to make that happen). The reference material for lists shows how to make sublists and more. 6.6.6 HTML Links Links have also already been exemplified. If you want the link text to be the same as the link URL, you can just put the URL in plain text. R Markdown will make it a link. It can also go in angle brackets. If you want the link text to be different from the link URL, you can just put the link text in square brackets followed by the URL in round brackets (with no space in between the two). For example, [CRAN](https://cran.r-project.org) makes the link CRAN. For more about links, here is the documentation. 6.6.7 Tables There is Markdown syntax for tables, but we won’t illustrate it here (documentation). We will be more interested in tables created by R. 6.7 Examples Involving R 6.7.1 Code Chunks Code chunks begin with ```{r} and end with ```. (documentation). The delimiters have to begin in column one (I think). Here is an example. 2 + 2 ## [1] 4 This is a “code chunk” processed by rmarkdown. When rmarkdown hits such a thing, it processes it, runs R to get the results, and stuffs the results (by default) in the output file it is creating. The text between code chunks is Markdown. 6.7.2 Code Chunks With Options Lots of things can be made to happen by including options in the code chunk beginning delimiter. AFAIK all knitr chunk options can be used. Here are some simple ones. The option eval=FALSE says to show the chunk but do not evaluate it, as here 2 + 2 The option echo=FALSE says to to not show the chunk but do evaluate it (just the opposite of eval=FALSE), as here (nothing appears in the output document because of echo=FALSE but the code chunk is executed). If you look at the document source you will see a hidden code chunk that assigns a value to the variable hide which we can see in a code chunk with no options hide ## [1] 3 This example also shows that all code chunks are executed in the same R session, so R objects assigned names in earlier chunks can be used in later chunks. Many more examples of options for code chunks are exemplified below. 6.7.3 Plots We showed a simple plot above, here is a more complicated one. 6.7.3.1 Make Up Data Simulate regression data, and do the regression. n &lt;- 50 x &lt;- seq(1, n) a.true &lt;- 3 b.true &lt;- 1.5 y.true &lt;- a.true + b.true * x s.true &lt;- 17.3 y &lt;- y.true + s.true * rnorm(n) out1 &lt;- lm(y ~ x) summary(out1) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -27.037 -15.248 0.169 9.651 31.364 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.235 4.780 1.30 0.2 ## x 1.348 0.163 8.26 8.9e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 16.6 on 48 degrees of freedom ## Multiple R-squared: 0.587, Adjusted R-squared: 0.579 ## F-statistic: 68.3 on 1 and 48 DF, p-value: 8.86e-11 6.7.3.2 Figure with Code to Make It Shown The following figure is produced by the following code plot(x, y) abline(out1) Figure 6.1: Simple Linear Regression Here we use the chunk options fig.align='center', fig.cap='Simple Linear Regression' to center the figure and to get the figure legend. Note that options are comma separated. 6.7.3.3 Figure with Code to Make It Not Shown For this example we do a cubic regression on the same data. out3 &lt;- lm(y ~ x + I(x^2) + I(x^3)) summary(out3) ## ## Call: ## lm(formula = y ~ x + I(x^2) + I(x^3)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -27.08 -14.75 -0.36 10.50 32.81 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.47e+01 1.02e+01 1.44 0.16 ## x 3.15e-01 1.71e+00 0.18 0.85 ## I(x^2) 2.37e-02 7.75e-02 0.31 0.76 ## I(x^3) -7.58e-05 9.99e-04 -0.08 0.94 ## ## Residual standard error: 16.6 on 46 degrees of freedom ## Multiple R-squared: 0.604, Adjusted R-squared: 0.579 ## F-statistic: 23.4 on 3 and 46 DF, p-value: 2.35e-09 Then we plot this figure with a hidden code chunk (so the R commands to make it do not appear in the document). Figure 6.2: Scatter Plot with Cubic Regression Curve This plot is made by a hidden code chunk that uses the option echo=FALSE in addition to fig.align and fig.caption that were also used in the preceding section. Also note that, as with the figure in the section titled What Does It Do? above, every time we rerun rmarkdown these two figures change because the simulated data are random. (We could use set.seed to make the simulated data always the same, if we wanted to.) 6.7.4 R in Text The no snarf and barf rule must be adhered to strictly. None at all! When you want to refer to some number in R printout, either make a code chunk that contains the printout you want, or, much nicer looking, you can “inline” R printout. Here we show how to do that. The quadratic and cubic regression coefficients in the preceding regression were 0.0237 and -7.584410^{-5}. Magic! See the source for this document to see how the magic works. If you never snarf and barf, and everything in your document is computed by R, then everything is always as claimed. 6.7.5 Tables The same goes for tables. Here is a “table” of sorts in some R printout. out2 &lt;- lm(y ~ x + I(x^2)) anova(out1, out2, out3) ## Analysis of Variance Table ## ## Model 1: y ~ x ## Model 2: y ~ x + I(x^2) ## Model 3: y ~ x + I(x^2) + I(x^3) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 48 13300 ## 2 47 12744 1 556 2.01 0.16 ## 3 46 12742 1 2 0.01 0.94 We want to turn that into a table in output format we are creating. First we have to figure out what the output of the R function anova is and capture it so we can use it. foo &lt;- anova(out1, out2, out3) class(foo) ## [1] &quot;anova&quot; &quot;data.frame&quot; So now we are ready to turn the matrix foo and the simplest way to do that seems to be the kable option on our R chunk Table 6.1: ANOVA Table Res.Df RSS Df Sum of Sq F Pr(&gt;F) 48 13300 47 12744 1 556 2.01 0.163 46 12742 1 2 0.01 0.940 6.8 LaTeX Math You can put real math in R Markdown documents. The way you do it mimics LaTeX (Wikipedia page), which is far and away the best document preparation system for ink on paper documents (it doesn’t work so well for e-readers). To actually learn LaTeX math, you have to read Section 3.3 of the LaTeX book and should also read the User’s Guide for the amsmath Package. Here are some examples to illustrate how good it is and how it works with R Markdown. \\[ f(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{- (x - \\mu)^2 / (2 \\sigma^2)}, \\qquad - \\infty &lt; x &lt; \\infty. \\] If \\[ f(x, y) = \\frac{1}{2}, \\qquad 0 &lt; x &lt; y &lt; 1, \\] then \\[\\begin{align} E(X) &amp; = \\int_0^1 d y \\int_0^y x f(x, y) \\, d x \\\\ &amp; = \\frac{1}{2} \\int_0^1 d y \\int_0^y x \\, d x \\\\ &amp; = \\frac{1}{2} \\int_0^1 d y \\left[ \\frac{x^2}{2} \\right]_0^y \\\\ &amp; = \\frac{1}{2} \\int_0^1 \\frac{y^2}{2} \\, d y \\\\ &amp; = \\frac{1}{2} \\cdot \\left[ \\frac{y^3}{6} \\right]_0^1 \\\\ &amp; = \\frac{1}{2} \\cdot \\frac{1}{6} \\\\ &amp; = \\frac{1}{12} \\end{align}\\] Unfortunately, to get this to work for HTML output requires (at least temporarily, until the rmarkdown package gets fixed) some arcane magic in the YAML header (just copy the one for this document). 6.9 Caching Computation If computations in an R Markdown take so much time that editing the document becomes annoying, you can “cache” the computations by adding the option cache=TRUE to time consuming code chunks. This feature is rather smart. Usually if anything changes in the input to the cached computations, the computations will be redone (for example if there were a change to the raw data), but if nothing has changed the computations will not be redone (the cached results will be used again) and no time is lost. So the computations are redone if and only if they need to be redone. My Stat 3701 course notes have an example that does caching: R Markdown source and HTML output. There are also other examples of lots of other things at http://www.stat.umn.edu/geyer/3701/notes/. 6.10 Summary Rmarkdown is terrific, so important that we cannot get along without it or its older competitors Sweave and knitr. Its virtues are The numbers and graphics you report are actually what they are claimed to be. Your analysis is reproducible. Even years later, when you’ve completely forgotten what you did, the whole write-up, every single number or pixel in a plot is reproducible. Your analysis actually works—at least in this particular instance. The code you show actually executes without error. Toward the end of your work, with the write-up almost done you discover an error. Months of rework to do? No! Just fix the error and rerun Rmarkdown. One single problem like this and you will have all the time invested in Rmarkdown repaid. This methodology provides discipline. There’s nothing that will make you clean up your code like the prospect of actually revealing it to the world. Whether we’re talking about homework, a consulting report, a textbook, or a research paper. If they involve computing and statistics, this is the way to do it. "],
["using-r-reproducibly.html", " 7 Using R Reproducibly 7.1 Types of Reproducibility 7.2 General Principles of Reproducibility 7.3 Starting a Reproducible Workflow in R", " 7 Using R Reproducibly Most scientific work is iterative source: Wickham &amp; Grolemund: R for Data Science The process between input and output needs to be transparent and repeatable. 7.1 Types of Reproducibility Many fields, most noteably biomedicine and psychology, have been undergoing a replication crisis - a name given to the finding that many published research results can not be reproduced. But what is meant by “reproducible”? Given the diversity of disciplines involved, not everyone uses the term in the same way. Scholars such as Victoria Stodden and Brian Nosek, who leads the Center for Open Science, distinguish between several types of reproducibility: Computational Reproducibility: Given the author’s data and statistical code, can someone produce the same results? Empirical Reproducibility: Is there enough information communicated about the study (e.g., design, experimental procedure, exclusion criteria, etc) for another researcher to repeat it in the exact same way? Statistical Reproducibility: Is adequate information provided about the choice of statistical tests, threshold values, corrections, and a priori hypotheses to protect against statistical biases (such as “p-hacking”)? Replicability: If someone re-runs the study with the same methods and analysis, collecting new data, do they get the same results? 7.2 General Principles of Reproducibility 7.2.1 Don’t do things by hand Everything that happens between the raw data and final output should be captured in the script. Point and click edits can’t be tracked, and it may be impossible to reverse any errors (rounding, copy/paste, overwriting). It can also be time consuming to re-do “by hand” edits. 7.2.2 Automate where you can Repetitive tasks are prone to errors (and are inefficient). As a programmatic language, R is useful for many automation tasks. File manipulation file.create(), dir.create(), list.files(), file.copy(), write() Looped processing for(), while() Vectorized functions apply(), lappy(), Vectorize() For example, our lab had an oscilloscope that collected data in around 100 separate .wav files. Before analysis, we needed to down sample, cut the time to the first five seconds, and remove the baseline each of the collected files. You could do this by opening each of the 100 files one at a time in an audio program, doing all the steps, and resaving (not so fun!). Or, automate with a few lines of R code: library(tuneR) setwd(&quot;~/Documents/wav_files&quot;) for (i in list.files()) { wavdata &lt;- readWave(paste0(getwd(), &quot;/&quot;, i), header=F) #take correct channel wavdata &lt;- ts(attr(wavdata, &quot;left&quot;), deltat=(1/64000)) #downsample wavdata &lt;- downsample(wavdata, samp.rate = 2004) #cut file wavdata &lt;- window(wavdata,start=1, end=5) #Take out the moving baseline with a spline wavdata &lt;- smooth.spline(x=wavdata, nknots=100) #save processed file save(wavdata, file=paste0(gsub(&quot;.wav&quot;, &quot;&quot;, i), &quot;_processed&quot;, &quot;.Rdata&quot;)) } 7.2.3 Use Version Control In addition to using tools such as Git and Github, think about how you will identify milestone versions of a project. Can you easily identify which version of your code and data was used for your submitted article? The revised article? A related conference presentation? 7.2.4 Document your environment source: tlvincent, Rbloggers; Data: CRAN archive The open source nature of R is what makes it a powerful tool for reproducible research, but it can also be part of the challenge. Although many packages allow backwards compatibility, not all do, and code run with one version of a package may not run (or worse, produce different results) than another. To ensure your code can be run in long after your current laptop is functioning, documentation is key. At a minimum, think about documenting: Infrastructure What software and hardware are you using to run your code? Packages What version of packages are used in your code? Workflow How did your data get to this script, where is it going? Functional requirements What is your script supposed to do? Capturing your session info does a lot of this for you: sessionInfo() ## R version 3.3.2 (2016-10-31) ## Platform: x86_64-apple-darwin13.4.0 (64-bit) ## Running under: macOS Sierra 10.12.5 ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods ## [7] base ## ## other attached packages: ## [1] car_2.1-4 knitr_1.16 RColorBrewer_1.1-2 ## [4] bindrcpp_0.2 dplyr_0.7.2 gridExtra_2.2.1 ## [7] ggplot2_2.2.1 fivethirtyeight_0.2.0 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_0.12.12 nloptr_1.0.4 plyr_1.8.4 ## [4] highr_0.6 bindr_0.1 tools_3.3.2 ## [7] lme4_1.1-12 digest_0.6.12 nlme_3.1-128 ## [10] lattice_0.20-34 jsonlite_1.4 evaluate_0.10 ## [13] tibble_1.3.3 gtable_0.2.0 mgcv_1.8-15 ## [16] pkgconfig_2.0.1 rlang_0.1.1 Matrix_1.2-7.1 ## [19] rstudioapi_0.6 parallel_3.3.2 yaml_2.1.14 ## [22] SparseM_1.77 stringr_1.2.0 MatrixModels_0.4-1 ## [25] htmlwidgets_0.8 nnet_7.3-12 rprojroot_1.2 ## [28] grid_3.3.2 DT_0.2 glue_1.1.1 ## [31] R6_2.2.2 rmarkdown_1.5 bookdown_0.4 ## [34] minqa_1.2.4 magrittr_1.5 splines_3.3.2 ## [37] MASS_7.3-45 backports_1.0.5 scales_0.4.1 ## [40] htmltools_0.3.6 pbkrtest_0.4-7 assertthat_0.2.0 ## [43] colorspace_1.3-2 quantreg_5.33 labeling_0.3 ## [46] stringi_1.1.5 lazyeval_0.2.0 munsell_0.4.3 7.2.5 Document your analysis While code is technically “self-documenting”, it doesn’t always make sense to another user (or yourself six months after you wrote it!). Comment code heavily, including instructions on the order different scripts should be executed. #Comment #Comment #Comment! Using a “literate programming” tool, such as R Markdown, that combines analysis and interpretation can make it easy to document your analysis as you go. 7.2.6 Make your data open One essential part of reproducibility is that your code (and data, where appropriate) are made available for verification or reuse. Many grants or journals may also require you to make your data and code available. There are many options for sharing: Github If you are a student, faculty, or staff at the University of Minnesota, you have access to two versions of Github: Pros of sharing with github: Version control/backup Easy to “open up” a private repo Great for reuse or contribution from others Cons of github: Clunky to point users to past milestone versions Limited project metadata (indexing, searchability) No formal archival actions Data &amp; Archival Repositories Data repositories are another option for sharing datasets and related documents, such as statistical code, procedures, or other supplemental material. These sites are specifically designed for hosting data, and many include services to make the data more findable, accessible, and preserved in the long-term. There are many different kinds of repositories for different kinds of data, and some journals or grants may require data to be shared in a specific repository Pros of data repositories: Enhance findability (metadata, Google Scholar indexing) Data citations, Digital Object Identifiers (DOIs) for persistent access May offer curation and archival services Some manage access to data, allowing access to data that may not be suitable for public access Cons of data repositories: Not every discipline has a dedicated repository Data services vary widely Can take a lot of work to prepare data for submission 7.3 Starting a Reproducible Workflow in R Want to get started with a more reproducible workflow in R? Here are some quick tips and suggestions for using good R style 7.3.1 Organization Create an R project for the main folder for a research project. This will make it easy to add git and github to your project. Use consistent directory organization in your project folder (e.g., separate folders for raw data, scripts, plots, reports), and reference the relative paths to these folders in your scripts. Give files descriptive and meaningful names. Avoid special characters (/ , . # ?), spaces between words, or very lengthy file names. 7.3.2 Anantomy of a script Order matters in a script; data used in an analysis needs to be loaded at an earlier point in the script. At the beginning of the script: Start with metadata (data about the script or dataset). This helps you quickly identify what the script does without pouring through the code. ########################################## ## Analysis Script for Project A ## 2017-09-01 ## ## Script takes in &quot;A_processed_data.csv&quot; ## Creates Tables 1-4, Figures 1, 2 for submission to ## Journal of Cool Results ######################################### Load all packages you use in one spot. Consider using package management tools such as pacman or packrat. pacman Makes it easy to load required packages, and install any that are missing. Very useful when sending scripts to collaborators. # &quot;p_load&quot; will load packages, installing any that are missing pacman::p_load(&quot;ggplot2&quot;, &quot;devtools&quot;, &quot;dplyr&quot;) packrat Creates a project-specific library for packages, so that the same versions of packages used in the script stay packaged with the script. #Set up a private package library for a project packrat::init(&quot;~/Documents/MyProject&quot;) Set the working directory at start, and use relative file paths throughout. If you do this through RStudio’s interface, copy the generated syntax into your script. setwd(&quot;~/Documents/MyProject&quot;) rawdata &lt;- read.csv(&quot;raw_data/Myrawdatafile.csv&quot;) Load all data in one spot. Again, if you are loading through the interface windows, copy the syntax generated in the console. Throughout your script: Use sections in R Studio to organize and quickly navigate your code. Press cmd-shift-R on a Mac or cntrl-shift-R on a PC to insert a section. Use lots of white space (it’s free). Use spaces between arguments and between different lines of code. # Good average &lt;- mean(feet / 12 + inches, na.rm = TRUE) # Bad average&lt;-mean(feet/12+inches,na.rm=TRUE) Comment extensively. #What does this do again? x[i][[1]] &lt;- do.call(rbind, y) Use sensible names for data objects and variables. Keep it simple but meaningful. # Good day_one day1 # Bad first_day_of_the_month DayOne dayone djm1 Set seed when doing any random number generations #take a random sample twice sample(1:100, size=10) ## [1] 66 49 90 13 60 26 98 51 97 83 sample(1:100, size=10) ## [1] 45 99 23 87 29 5 9 59 86 66 #set seed before hand to ensure the random numbers are consistent set.seed(seed=42) sample(1:100, size=10) ## [1] 92 93 29 81 62 50 70 13 61 65 set.seed(seed=42) sample(1:100, size=10) ## [1] 92 93 29 81 62 50 70 13 61 65 If you save any outputs, make sure they are done by your script. ggsave(), png(), pdf() # Base R: Creates &quot;Figure1.png&quot; in the working directory png(file=&quot;Figure1.png&quot;) ggplot(election, aes(x=winrep_2016)) + geom_bar() + labs(x=&quot;Trump win&quot;, y=&quot;Number of counties&quot;) dev.off() #ggplot2: Creates &quot;Figure1.png&quot; in working directory g &lt;- ggplot(election, aes(x=winrep_2016)) + geom_bar() + labs(x=&quot;Trump win&quot;, y=&quot;Number of counties&quot;) ggsave(filename=&quot;Figure1.png&quot;, plot=g, device=&quot;png&quot;) At the end of your R session Do not save your “R workspace” at the end of your R sessions. The R workspace contains all the objects currently loaded in your R environment. Instead, your script should generate all the objects needed. You can set R options to ensure it does not get saved. You can also clear your environment at any time with the broom icon in the Environment window. Clear the console in RStudio with cntl-l. "]
]
